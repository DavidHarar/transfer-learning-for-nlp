{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Open Ended Text Generation with GPT-2"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Pipeline uses `gpt2` by default, but we specify it explicitly to be fully transparent\nfrom transformers import pipeline\ngpt = pipeline('text-generation',model='gpt2')","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b82069bf188494f9a5770a746e9acea"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618814c68eb94624962c6e8a369c8824"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514a7d2e8bbd41f4be9672b5d543aed3"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85dc5184b6094ca19cc0347395bda0aa"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42796ea7e9c44be58e9d240796738afb"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now, let's generate some text with GPT-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"gpt(\"Transfer learning is a field of study\", max_length=100)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"[{'generated_text': \"Transfer learning is a field of study that has been around for centuries, and one that requires a thorough grounding in mathematics in order to understand the complexities of these systems. If you go to the library for your high school physics course, you know you're on the right track. The only problem with this position is that people don't ask questions. The only thing they really do ask is: how do we figure out how to apply these processes to the rest of physics and other sciences?\\n\\nIn\"}]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"A nonexhaustive list of other model choices suitable for text generation within the transformers library include \"ctrl\" (CTRL - huge! too big for Kaggle), \"xlnet-base-cased\" (XLNet), \"transfo-xl-wt103\" (Transformer XL)... These often need to be padded very carefully to work well, GPT-2 is the safest choice for open-ended text generation. See https://huggingface.co/transformers/usage.html#text-generation for more."},{"metadata":{},"cell_type":"markdown","source":"# Conversational Text Generation with DialoGPT\n\nDialoGPT is an extension of GPT to conversational response generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer # you can use these utility classes that automatically load the right classes\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer # or these more specific classes directly\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\")","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872f3285e76647c39b2f2799a4cb7ed4"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605205a1d7624b72adb5d3a4ea28507d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=555.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6caa110b4abb4df8a131b207dc1c2b25"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=862955157.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0099bce0c1443e1b7440f446f8c0f1b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chat for 5 Lines\nconversation_length = 5\nfor step in range(conversation_length):\n    # encode new user input, add end-of-sentence token, return tensor\n    new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors='pt')\n    \n    # add new input to chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1) if step > 0 else new_user_inputs_ids\n    \n    # generate a response of up to max_length tokens\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    \n    # display response\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"User: hi robot\nDialogGPT: Hello, human.\nUser: huh?\nDialogGPT: I'm a bot.\nUser: ok, what is your name?\nDialogGPT: Robot. I'm a robot.\nUser: Alright then\nDialogGPT: Robot. I'm a robot.\nUser: Say something else\nDialogGPT: Robot. I'm a robot.\n"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}