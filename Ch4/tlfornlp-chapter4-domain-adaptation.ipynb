{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","metadata":{}},{"cell_type":"markdown","source":"# Preliminaries\nInstall required dependencies not already on the Kaggle image","metadata":{}},{"cell_type":"code","source":"# install sent2vec\n!pip install git+https://github.com/epfml/sent2vec","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/epfml/sent2vec\n  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-pk9bc4tp\n  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-pk9bc4tp\nBuilding wheels for collected packages: sent2vec\n  Building wheel for sent2vec (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1139414 sha256=c1870293d604840a0617afa1a0cbcf9ffb07f5a4157f8d90fe59229c0e71fda8\n  Stored in directory: /tmp/pip-ephem-wheel-cache-srnnopbd/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\nSuccessfully built sent2vec\nInstalling collected packages: sent2vec\nSuccessfully installed sent2vec-0.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Write requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp","metadata":{}},{"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Download IMDB Movie Review Dataset\nDownload IMDB dataset","metadata":{}},{"cell_type":"code","source":"import random\nimport pandas as pd\n\n## Read-in the reviews and print some basic descriptions of them\n\n!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n!tar xzf aclImdb_v1.tar.gz","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Tokenization, Stop-word and Punctuation Removal Functions\nBefore proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters","metadata":{}},{"cell_type":"markdown","source":"**Preliminary overarching hyperparameters**\n","metadata":{}},{"cell_type":"code","source":"Nsamp = 1000 # number of samples to generate in each class - 'positive', 'negative'\nmaxtokens = 200 # the maximum number of tokens per document\nmaxtokenlen = 100 # the maximum length of each token","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Tokenization**","metadata":{}},{"cell_type":"code","source":"def tokenize(row):\n    if row is None or row is '':\n        tokens = \"\"\n    else:\n        tokens = str(row).split(\" \")[:maxtokens]\n    return tokens","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Use regular expressions to remove unnecessary characters**\n\nNext, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef reg_expressions(row):\n    tokens = []\n    try:\n        for token in row:\n            token = token.lower()\n            token = re.sub(r'[\\W\\d]', \" \", token)\n            token = token[:maxtokenlen] # truncate token\n            tokens.append(token)\n    except:\n        token = \"\"\n        tokens.append(token)\n    return tokens","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Stop-word removal**\n\nStop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings.","metadata":{}},{"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')    \n\n# these stopwords may indicate positivity/negativity of sentiment, so we remove them (keep them in corpus)\n# stopwords.remove(\"no\")\n# stopwords.remove(\"nor\")\n# stopwords.remove(\"not\")\n\n# print(stopwords) # see default stopwords\n","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}]},{"cell_type":"code","source":"def stop_word_removal(row):\n    token = [token for token in row if token not in stopwords]\n    token = filter(None, token)\n    return token","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Assemble Embedding Vectors\n\nThe following functions are used to extract sent2vec embedding vectors for each review","metadata":{}},{"cell_type":"code","source":"import time\nimport sent2vec\n\ns2v_model = sent2vec.Sent2vecModel()\nstart=time.time()\ns2v_model.load_model('../input/sent2vec/wiki_unigrams.bin')\nend = time.time()\nprint(\"Loading the sent2vec embedding took %d seconds\"%(end-start))","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Loading the sent2vec embedding took 50 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def assemble_embedding_vectors(data):\n    out = None\n    for item in data:\n        vec = s2v_model.embed_sentence(\" \".join(str(i) for i in item if i))\n        if vec is not None:\n            if out is not None:\n                out = np.concatenate((out,vec),axis=0)\n            else:\n                out = vec                                            \n        else:\n            pass\n        \n        \n    return out","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Putting It All Together To Assemble Dataset\n\nNow, putting all the preprocessing steps together we assemble our dataset...","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\n# shuffle raw data first\ndef unison_shuffle_data(data, header):\n    p = np.random.permutation(len(header))\n    data = data[p]\n    header = np.asarray(header)[p]\n    return data, header\n\ndef load_data(path):\n    data, sentiments = [], []\n    for folder, sentiment in (('neg', 0), ('pos', 1)):\n        folder = os.path.join(path, folder)\n        for name in os.listdir(folder):\n            with open(os.path.join(folder, name), 'r') as reader:\n                  text = reader.read()\n            text = tokenize(text)\n            text = stop_word_removal(text)\n            text = reg_expressions(text)\n            data.append(text)\n            sentiments.append(sentiment)\n    data_np = np.array(data)\n    data, sentiments = unison_shuffle_data(data_np, sentiments)\n    \n    return data, sentiments\n\ntrain_path = os.path.join('aclImdb', 'train')\ntest_path = os.path.join('aclImdb', 'test')\nraw_data, raw_header = load_data(train_path)\n\nprint(raw_data.shape)\nprint(len(raw_header))","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(25000,)\n25000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Subsample required number of samples\nrandom_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\ndata_train = raw_data[random_indices]\nheader = raw_header[random_indices]\n\ndel raw_data, raw_header # huge and no longer needed, get rid of it\n\nprint(\"DEBUG::data_train::\")\nprint(data_train)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"DEBUG::data_train::\n[list(['this', 'one', 'best', 'movies', 'serial', 'killers', 'i ve', 'ever', 'seen ', 'coming', 'someone', 'absolutely', 'loved', 'silence', 'lambs ', 'hbo', 'hit', 'jackpot', 'here ', 'this', 'film', 'compelling', 'first', 'moment', 'last  br', '   br', '  this', 'film', 'many', 'underlying', 'themes', 'hard', 'tell', 'exactly', 'about ', 'it', 'chronicles', 'decade long', 'search', 'russian', 'serial', 'killer', 'andrea', 'chikatilo ', 'stephen', 'rea', 'gives', 'brilliantly', 'reserved', 'performance', 'inexperienced', 'forensic', 'expert', 'put', 'charge', 'investigation ', 'donald', 'sutherland', 'gives', 'even', 'involving', 'performance', 'cynical', 'superior ', 'person', 'russian', 'government', 'willing', 'help', 'him ', 'both', 'performances', 'subtle', 'masterpieces   rea', 'begins', 'naive', 'unwilling', 'compromise ', 'sutherland', 'begins', 'detached', 'almost', 'amused', 'situation ', 'towards', 'end ', 'rea', 'becomes', 'world weary', 'beaten', 'system ', 'sutherland', 'finds', 'passionate', 'idealistic  br', '   br', '  in', 'movie ', 'i', 'would', 'said', 'sutherland s', 'performance', 'stands', 'rest ', 'even', 'rivaled', 'jeffrey', 'dumann ', 'serial', 'killer', 'himself ', 'dumann', 'brilliantly', 'creates', 'character'])\n list(['story', 'good for nothing', 'poet', 'sidekick', 'singer', 'puts', 'words', 'music ', 'director', 'danny', 'boyle', 'lost', 'none', 'predilection', 'raking', 'gutter', 'humanity', 'characters', 'lost ', 'film ', 'edge', 'creating', 'inspiring', 'funny', 'films ', 'strumpet', 'painful', 'watch', 'barely', 'justified', 'fact', 'made', 'tv '])\n list(['not', 'bad ', 'credited', ' hooper s', 'done', 'far', 'worse  ', 'disappointing', 'me ', 'such', 'imaginative', 'concept ', 'never', 'really', 'tapped', 'hooper', 'economical', 'direction', 'even', 'less', 'smoky', ' excuse', 'pun ', 'writing ', 'it', 'goes', 'sinister', 'over the top', 'dead', 'serious', 'tone ', 'becoming', 'ridiculous', 'unfocused', 'letting', 'whole', 'pessimistic', 'mystery', ' ', 'conspiracy laced', 'narrative', 'easily', 'telegraphed', 'end', 'something', 'completely', 'abrupt ', 'because', 'that ', 'pacing', 'goes', 'rather', 'sluggish', 'brad', 'dourif', ' cool', 'see', 'leading', 'role ', 'seems', 'struggle', 'off balanced', 'performance ', 'despite', 'etching', 'bemusedly', 'quirky', 'intensity', 'off colour', 'character ', 'even', 'though', 'cheaply', 'done ', 'there s', 'competent', 'technical', 'attitude', 'it ', 'however', 'seem', 'go', 'anywhere', 'ordinary', 'idea', 'wants', 'plaster', 'nasty', 'jolts', ' which', 'work ', 'strikingly', 'steaming', 'special', 'effects', ' flames ', 'flames', 'everywhere ', 'instead ', 'hooper', 'display', 'stylishly', 'frenetic', 'imagery', ' more', 'towards', 'latter', 'end  ', 'camera work', 'swiftly', 'manoeuvred', 'beaming', 'score', 'titillating ', 'the', 'performances', 'bit', 'shop', 'appearances'])\n ...\n list(['in', '     ', ' the', 'visitors ', 'enormous', 'hit', 'france ', 'so ', 'sequence', 'inevitable', 'unfortunately ', 'sequence', 'ranks', 'among', 'worst', 'ones', 'ever', 'made ', ' br', '   br', '  this', 'movie', 'keep', 'promises ', 'indeed ', 'supposed', 'tell', 'sole', 'story ', 'jean', 'reno', 'must', 'go', 'twentieth', 'century', 'take', 'christian', 'clavier', 'back', 'middle', 'ages', 'time', 'normally', 'follow', 'course ', 'the', 'problem', 'clavier', 'feels', 'completely', 'ease', 'world', 'twentieth', 'century ', 'make', 'get', 'back', 'middles', 'ages', 'rather', 'hard   ', 'instead', 'this ', 'movie', 'goes', 'several', 'stories', 'without', 'succeeding', 'following', 'main', 'plot ', 'as', 'consequence ', 'movie', 'becomes', 'sometimes', 'muddle headed ', 'sometimes', 'bit', 'mess  br', '   br', '  but', 'movie', 'also', 'suffers', 'performance', 'nearly', 'actors ', 'reno', 'clavier', 'fall', 'trap', 'however', 'could', 'avoid', 'first', 'movie ', 'they re', 'going', 'top', 'become', 'annoying ', 'then ', 'jean marie', 'poiré', 'film maker', 'engage', 'muriel', 'robin', 'female', 'main', 'role ', 'he', 'made', 'mistake', 'seems', 'ill at ease', 'absolutely', 'pitiful ', 'the', 'actors', 'better '])\n list(['i', 'say', 'i', 'really', 'looking', 'forward', 'watching', 'film', 'finding', 'new', 'life', 'would', 'separate', 'dull', 'overly', 'crafted', 'mexican', 'films ', 'i', 'idea', 'i', 'trusted', 'sexo ', 'pudor', 'lagrimas', 'one', 'inject', 'freshness', 'confidence', 'non existent', 'industry ', 'maybe', 'soundtrack which', 'i', 'listened', 'i', 'saw', 'film ', 'sounded', 'different', 'others ', 'maybe', 'dared', 'include', 'newer', 'faces apart', 'demian', 'bichir', 'always', 'favorite', 'mexican', 'film', 'directors ', 'supposedly', 'dealed', 'within', 'script', 'modern', 'social', 'behaviour ', 'maybe', 'photography', 'i', 'saw', 'trailers', 'bright', 'realistic', 'instead', 'theatrical ', 'the', 'film', 'turned', 'major', 'crowd', 'pleaser ', 'major', 'letdown ', 'what', 'serrano', 'actually', 'deals', 'old', 'fashioned', ' battle', 'sexes ', ' all', 'men', 'same ', ' why', 'women     ', 'blah blah blah ', 'nothing', 'new', 'it ', 'even', 'that ', 'uses', 'much', 'common', 'ground', 'clichè', 'eventually', 'mocks', 'without', 'leaving', 'valuable', 'reflexion', 'female male', 'condition ', 'full', 'usual', 'tramps'])\n list(['as', 'i', 'always', 'looking', 'something', 'new', 'unique ', 'i', 'watched', 'film', 'online ', 'i', 'thought', 'would', 'another', ' b ', 'rate', 'movie', 'i', 'amazed', 'acting', 'two', 'main', 'characters ', 'all', 'actors', 'film', 'capable', 'well', 'directed ', 'the', 'plot', 'wonderful', 'unique', 'well', 'excellent', 'moral', 'story  br', '   br', '  this', 'movie', 'definitely', 'someone', 'looking', 'sex', 'romp ', ' dumb', 'dumber ', 'blood', 'guts ', 'this', 'wonderfully', 'poignant', 'film', 'showing', 'grim', 'realities', 'life', 'coupled', 'kindness', 'human', 'heart', 'enough', 'frivolity', 'keep', 'interesting  br', '   br', '  i', 'would', 'prefer', 'movie', 'many', ' a ', 'rate', 'movies', 'i', 'seen', 'even', 'great', 'number', 'high', 'box', 'office', 'earnings  br', '   br', '  i', 'highly', 'recommend', 'movie '])]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes","metadata":{}},{"cell_type":"code","source":"unique_elements, counts_elements = np.unique(header, return_counts=True)\nprint(\"Sentiments and their frequencies:\")\nprint(unique_elements)\nprint(counts_elements)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Sentiments and their frequencies:\n[0 1]\n[ 993 1007]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Featurize and Create Labels**","metadata":{}},{"cell_type":"code","source":"EmbeddingVectors = assemble_embedding_vectors(data_train)\nprint(EmbeddingVectors)","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[[-0.03622827  0.01441378 -0.05925672 ... -0.00478834 -0.04687782\n   0.0717904 ]\n [-0.05283374 -0.14769883 -0.02701941 ...  0.15918025  0.16663636\n   0.18797083]\n [ 0.13212578 -0.15448087 -0.04736134 ... -0.05063647 -0.027518\n   0.20125072]\n ...\n [-0.05686692 -0.11101679  0.10533109 ...  0.07239283  0.00357683\n   0.09122648]\n [ 0.08307935 -0.12593347  0.00109499 ...  0.08826873 -0.07179418\n   0.06714546]\n [ 0.02595801 -0.05723677  0.07862083 ... -0.03257715 -0.07341827\n   0.0762985 ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"data = EmbeddingVectors\n\nidx = int(0.7*data.shape[0])\n\n# 70% of data for training\ntrain_x = data[:idx,:]\ntrain_y = np.array(header[:idx])\n# # remaining 30% for testing\ntest_x = data[idx:,:]\ntest_y = np.array(header[idx:]) \n\nprint(\"train_x/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\nprint(train_x)\nprint(train_y[:5])\nprint(len(train_y))","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"train_x/train_y list details, to make sure it is of the right form:\n1400\n[[-0.03622827  0.01441378 -0.05925672 ... -0.00478834 -0.04687782\n   0.0717904 ]\n [-0.05283374 -0.14769883 -0.02701941 ...  0.15918025  0.16663636\n   0.18797083]\n [ 0.13212578 -0.15448087 -0.04736134 ... -0.05063647 -0.027518\n   0.20125072]\n ...\n [ 0.02726981  0.02248825 -0.21001302 ... -0.04563235 -0.27397254\n   0.17307606]\n [-0.01805624 -0.09111983 -0.10452412 ... -0.1359355  -0.04638012\n   0.12248638]\n [ 0.01417789 -0.11241481 -0.13324331 ... -0.07542464  0.00514632\n   0.06470316]]\n[1 0 0 1 0]\n1400\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Shallow Model for IMDB Reviews","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\n\ninput_shape = (len(train_x[0]),)\nsent2vec_vectors = Input(shape=input_shape)\ndense = Dense(512, activation='relu')(sent2vec_vectors)\ndense = Dropout(0.1)(dense)\noutput = Dense(1, activation='sigmoid')(dense)\nmodel = Model(inputs=sent2vec_vectors, outputs=output)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n                  optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32,\n                    nb_epoch=10, shuffle=True)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  after removing the cwd from sys.path.\n","output_type":"stream"},{"name":"stdout","text":"Train on 1400 samples, validate on 600 samples\nEpoch 1/10\n1400/1400 [==============================] - 1s 472us/step - loss: 0.5553 - accuracy: 0.7243 - val_loss: 0.4703 - val_accuracy: 0.7733\nEpoch 2/10\n1400/1400 [==============================] - 0s 205us/step - loss: 0.4028 - accuracy: 0.8264 - val_loss: 0.4509 - val_accuracy: 0.7767\nEpoch 3/10\n1400/1400 [==============================] - 0s 194us/step - loss: 0.3635 - accuracy: 0.8400 - val_loss: 0.4543 - val_accuracy: 0.7783\nEpoch 4/10\n1400/1400 [==============================] - 0s 204us/step - loss: 0.3383 - accuracy: 0.8600 - val_loss: 0.4809 - val_accuracy: 0.7783\nEpoch 5/10\n1400/1400 [==============================] - 0s 208us/step - loss: 0.3190 - accuracy: 0.8671 - val_loss: 0.4630 - val_accuracy: 0.7917\nEpoch 6/10\n1400/1400 [==============================] - 0s 212us/step - loss: 0.2801 - accuracy: 0.8893 - val_loss: 0.4530 - val_accuracy: 0.7850\nEpoch 7/10\n1400/1400 [==============================] - 0s 211us/step - loss: 0.2585 - accuracy: 0.9036 - val_loss: 0.4550 - val_accuracy: 0.7967\nEpoch 8/10\n1400/1400 [==============================] - 0s 199us/step - loss: 0.2495 - accuracy: 0.9079 - val_loss: 0.4778 - val_accuracy: 0.7867\nEpoch 9/10\n1400/1400 [==============================] - 0s 195us/step - loss: 0.2186 - accuracy: 0.9229 - val_loss: 0.5078 - val_accuracy: 0.7833\nEpoch 10/10\n1400/1400 [==============================] - 0s 206us/step - loss: 0.2025 - accuracy: 0.9286 - val_loss: 0.5271 - val_accuracy: 0.7767\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test Trained Model on Book Reviews from MDSD","metadata":{}},{"cell_type":"code","source":"!ls ../input/multi-domain-sentiment-dataset-books-and-dvds/","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"books.negative.review  dvd.negative.review\nbooks.positive.review  dvd.positive.review\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_MDSD(data):\n    out_lst = []\n    for i in range(len(data)):\n        txt = \"\"\n        if(data[i]==\"<review_text>\\n\"):\n            j=i\n            while(data[j]!=\"</review_text>\\n\"):\n                txt = txt+data[j]\n                j = j+1\n            text = tokenize(txt)\n            text = stop_word_removal(text)\n            text = reg_expressions(text)\n            out_lst.append(text)\n            \n            #print(txt)\n            #print(text)\n            \n    return out_lst\n\nwith open (\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.negative.review\", \"r\", encoding=\"latin1\") as myfile:\n    data=myfile.readlines()\nneg_books = parse_MDSD(data)\nlen(neg_books)\n\nwith open (\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.positive.review\", \"r\", encoding=\"latin1\") as myfile:\n    data=myfile.readlines()\npos_books = parse_MDSD(data)\nlen(pos_books)\n\n#print(neg_books)\n#print(pos_books)\n\nheader = [0]*len(neg_books)\nheader.extend([1]*len(pos_books))\nneg_books.extend(pos_books)\nMDSD_data = np.array(neg_books)\n\ndata, sentiments = unison_shuffle_data(np.array(MDSD_data), header)\n\nlen(sentiments)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"2000"},"metadata":{}}]},{"cell_type":"markdown","source":"**Try using the IMDB classifier directly on book review data...**","metadata":{}},{"cell_type":"code","source":"EmbeddingVectors = assemble_embedding_vectors(data)\nprint(EmbeddingVectors)\nsentiments = np.asarray(sentiments)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[[ 6.9606602e-02 -3.0756295e-02 -2.3950104e-01 ... -2.2783343e-01\n  -3.7275681e-01  4.9442065e-01]\n [-1.2904549e-02 -1.1575852e-01 -1.5874037e-01 ...  5.3884499e-02\n  -2.0341949e-01  1.4110129e-01]\n [ 1.0849423e-01 -3.8145301e-01 -8.1480257e-02 ... -3.9360296e-02\n   2.0374845e-01  2.6633894e-01]\n ...\n [-2.0717914e-01 -4.0148088e-01  1.2224840e-01 ... -4.1686586e-04\n  -1.5059590e-01  1.6732480e-01]\n [-1.3965216e-01 -7.1579702e-02 -2.5804830e-01 ...  1.0827243e-01\n  -3.4484950e-01  2.5415501e-01]\n [-1.5061742e-01 -4.2240743e-02 -3.8007267e-02 ...  3.8678069e-02\n   5.0857328e-03  9.2025444e-02]]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nprint(model.evaluate(x=EmbeddingVectors,y=sentiments)) # evaluate IMDB classifier on books directly\nprint(model.metrics_names)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"2000/2000 [==============================] - 0s 43us/step\n[0.6844062490463256, 0.7289999723434448]\n['loss', 'accuracy']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Adaptation of Book Review Domain via Autoencoder","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nencoding_dim = 30 # chosen experimentally\n\ninput_shape = (len(train_x[0]),)\nsent2vec_vectors = Input(shape=input_shape)\nencoder = Dense(encoding_dim)(sent2vec_vectors)\ndropout = Dropout(0.1)(encoder)\ndecoder = Dense(encoding_dim)(dropout)\ndropout = Dropout(0.1)(decoder)\noutput = Dense(len(train_x[0]))(dropout)\nautoencoder = Model(inputs=sent2vec_vectors, outputs=output)","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam',loss='mse',metrics=[\"mse\",\"mae\"])\nautoencoder.fit(train_x,train_x,validation_data=(test_x, test_x), batch_size=32,\n                    nb_epoch=50, shuffle=True)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  This is separate from the ipykernel package so we can avoid doing imports until\n","output_type":"stream"},{"name":"stdout","text":"Train on 1400 samples, validate on 600 samples\nEpoch 1/50\n1400/1400 [==============================] - 0s 320us/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0938 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0757\nEpoch 2/50\n1400/1400 [==============================] - 0s 89us/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0751 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0716\nEpoch 3/50\n1400/1400 [==============================] - 0s 86us/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0715 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0693\nEpoch 4/50\n1400/1400 [==============================] - 0s 79us/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0694 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0675\nEpoch 5/50\n1400/1400 [==============================] - 0s 82us/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0678 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0664\nEpoch 6/50\n1400/1400 [==============================] - 0s 80us/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0666 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0655\nEpoch 7/50\n1400/1400 [==============================] - 0s 79us/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0658 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0647\nEpoch 8/50\n1400/1400 [==============================] - 0s 87us/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0650 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0643\nEpoch 9/50\n1400/1400 [==============================] - 0s 88us/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0646 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0636\nEpoch 10/50\n1400/1400 [==============================] - 0s 79us/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0640 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0632\nEpoch 11/50\n1400/1400 [==============================] - 0s 83us/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0635 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0629\nEpoch 12/50\n1400/1400 [==============================] - 0s 79us/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0632 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0626\nEpoch 13/50\n1400/1400 [==============================] - 0s 83us/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0629 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0623\nEpoch 14/50\n1400/1400 [==============================] - 0s 86us/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0627 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0621\nEpoch 15/50\n1400/1400 [==============================] - 0s 87us/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0624 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0619\nEpoch 16/50\n1400/1400 [==============================] - 0s 88us/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0622 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0617\nEpoch 17/50\n1400/1400 [==============================] - 0s 91us/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0619 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0615\nEpoch 18/50\n1400/1400 [==============================] - 0s 88us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0618 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0614\nEpoch 19/50\n1400/1400 [==============================] - 0s 89us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0617 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0612\nEpoch 20/50\n1400/1400 [==============================] - 0s 89us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0615 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0610\nEpoch 21/50\n1400/1400 [==============================] - 0s 79us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0613 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0610\nEpoch 22/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0612 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0608\nEpoch 23/50\n1400/1400 [==============================] - 0s 85us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0610 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0607\nEpoch 24/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0610 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0606\nEpoch 25/50\n1400/1400 [==============================] - 0s 93us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0609 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0604\nEpoch 26/50\n1400/1400 [==============================] - 0s 96us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0608 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0604\nEpoch 27/50\n1400/1400 [==============================] - 0s 88us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0603\nEpoch 28/50\n1400/1400 [==============================] - 0s 88us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0606 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0602\nEpoch 29/50\n1400/1400 [==============================] - 0s 89us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0605 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0602\nEpoch 30/50\n1400/1400 [==============================] - 0s 86us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0604 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0600\nEpoch 31/50\n1400/1400 [==============================] - 0s 84us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0603 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0600\nEpoch 32/50\n1400/1400 [==============================] - 0s 78us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0603 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0599\nEpoch 33/50\n1400/1400 [==============================] - 0s 80us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0598\nEpoch 34/50\n1400/1400 [==============================] - 0s 84us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0602 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0598\nEpoch 35/50\n1400/1400 [==============================] - 0s 84us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0597\nEpoch 36/50\n1400/1400 [==============================] - 0s 86us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0600 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0596\nEpoch 37/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0596\nEpoch 38/50\n1400/1400 [==============================] - 0s 85us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0599 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0595\nEpoch 39/50\n1400/1400 [==============================] - 0s 77us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0599 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0595\nEpoch 40/50\n1400/1400 [==============================] - 0s 82us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0598 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0594\nEpoch 41/50\n1400/1400 [==============================] - 0s 78us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0598 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0594\nEpoch 42/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0597 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0593\nEpoch 43/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0596 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0592\nEpoch 44/50\n1400/1400 [==============================] - 0s 82us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0595 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0593\nEpoch 45/50\n1400/1400 [==============================] - 0s 82us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0596 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0592\nEpoch 46/50\n1400/1400 [==============================] - 0s 81us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0595 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0592\nEpoch 47/50\n1400/1400 [==============================] - 0s 82us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0595 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0591\nEpoch 48/50\n1400/1400 [==============================] - 0s 76us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0595 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0590\nEpoch 49/50\n1400/1400 [==============================] - 0s 84us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0591\nEpoch 50/50\n1400/1400 [==============================] - 0s 78us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0590\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7efadb0e9b38>"},"metadata":{}}]},{"cell_type":"code","source":"# transform EmbeddingVectors and sentiments with autoencoder.predict and then evaluate IDMB model again\n\nEmbeddingVectorsProjected = autoencoder.predict(EmbeddingVectors)\n\nprint(EmbeddingVectorsProjected.shape)\n\nprint(model.evaluate(x=EmbeddingVectorsProjected,y=sentiments))\nprint(model.metrics_names)","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"(2000, 600)\n2000/2000 [==============================] - 0s 43us/step\n[0.5985191793441772, 0.7354999780654907]\n['loss', 'accuracy']\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n#create_download_link(filename='GBMimportances.svg')","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!rm -rf aclImdb\n!rm aclImdb_v1.tar.gz","metadata":{"trusted":true},"execution_count":26,"outputs":[]}]}