{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "Install required dependencies not already on the Kaggle image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/epfml/sent2vec\r\n",
      "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-6usf0w9e\r\n",
      "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-6usf0w9e\r\n",
      "Building wheels for collected packages: sent2vec\r\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1139408 sha256=5d41d4ea2c3b1e451e3ee71fae31af75b6747a9ae33021dee09a8ed06e040cad\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8nd6mv10/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\r\n",
      "Successfully built sent2vec\r\n",
      "Installing collected packages: sent2vec\r\n",
      "Successfully installed sent2vec-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "# install sent2vec\n",
    "!pip install git+https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n",
    "\n",
    "Requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download IMDB Movie Review Dataset\n",
    "Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "## Read-in the reviews and print some basic descriptions of them\n",
    "\n",
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization, Stop-word and Punctuation Removal Functions\n",
    "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use regular expressions to remove unnecessary characters**\n",
    "\n",
    "Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower() # make all characters lower case\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop-word removal**\n",
    "\n",
    "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "# print(stopwords) # see default stopwords\n",
    "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
    "# of a sentence\n",
    "# stopwords.remove(\"no\")\n",
    "# stopwords.remove(\"nor\")\n",
    "# stopwords.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Embedding Vectors\n",
    "The following functions are used to extract sent2vec embedding vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sent2vec embedding took 43 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sent2vec\n",
    "\n",
    "s2v_model = sent2vec.Sent2vecModel()\n",
    "start=time.time()\n",
    "s2v_model.load_model('../input/sent2vec/wiki_unigrams.bin')\n",
    "end = time.time()\n",
    "print(\"Loading the sent2vec embedding took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_embedding_vectors(data):\n",
    "    out = None\n",
    "    for item in data:\n",
    "        vec = s2v_model.embed_sentence(\" \".join(item))\n",
    "        if vec is not None:\n",
    "            if out is not None:\n",
    "                out = np.concatenate((out,vec),axis=0)\n",
    "            else:\n",
    "                out = vec                                            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together To Assemble Dataset\n",
    "Now, putting all the preprocessing steps together we assemble our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "# load data in appropriate form\n",
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    \n",
    "    return data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::data_train::\n",
      "[list(['there', 'interesting', 'discussion', 'movie', 'is', 'moral', 'person', 'good', 'enough', 'need', 'something', 'morebr', 'br', 'the', 'movie', 'preaches', 'without', 'guidance', 'god', 'morally', 'good', 'person', 'enough', 'there', 'line', 'early', 'movie', 'you', 'i', 'look', 'person', 'morally', 'good', 'know', 'going', 'go', 'hellbr', 'br', 'while', 'i', 'christian', 'discussions', 'throughout', 'course', 'movie', 'fascinating', 'way', 'movie', 'intended', 'i', 'left', 'movie', 'stronger', 'feeling', 'morally', 'good', 'is', 'enough', 'the', 'arguments', 'discussions', 'presented', 'heavily', 'biased', 'much', 'crush', 'weight', 'ignorance', 'fanaticism', 'powerful', 'thing', 'especially', 'inferenced', 'minds', 'ignorant', 'uneducated', 'as', 'george', 'carlins', 'character', 'dogma', 'said', 'hook', 'em', 'theyre', 'youngbr', 'br', 'the', 'basic', 'premise', 'interesting', 'one', 'also', 'a', 'bible', 'scholar', 's', 'attempting', 'publish', 'book', 'says', 'morality', 'without', 'god', 'ok', 'long', 'morality', 'meaningful', 'do'])\n",
      " list(['the', 'th', 'animated', 'disney', 'classic', 'reasonable', 'movie', 'told', 'simple', 'story', 'even', 'though', 'little', 'dated', 'deserves', 'place', 'list', 'disney', 'classicsbr', 'br', 'its', 'among', 'disneys', 'top', 'works', 'satisfying', 'one', 'disneys', 'simple', 'works', 'yes', 'keeps', 'certain', 'magic', 'enchantment', 'which', 'old', 'disney', 'well', 'known', 'for', 'this', 'important', 'movie', 'saved', 'disney', 'delicate', 'situation', 'if', 'failure', 'disney', 'animated', 'classicsbr', 'br', 'cinderella', 'somehow', 'like', 'return', 'disneys', 'st', 'animated', 'classic', 'snow', 'white', 'seven', 'dwarfs', 'brings', 'back', 'fairy', 'tale', 'genre', 'its', 'clear', 'story', 'takes', 'place', 'i', 'suppose', 'somewhere', 'france', 'based', 'tale', 'charles', 'perraultbr', 'br', 'there', 'plenty', 'likable', 'characters', 'cinderella', 'prince', 'bruno', 'the', 'dog', 'jaques', 'gus', 'the', 'two', 'main', 'mice', 'fairy', 'godmother', 'for', 'fairy', 'sure', 'funny', 'birds', 'king', 'grand', 'dukebr', 'br', 'jaques', 'smart', 'amusing', 'i', 'love', 'voice', 'really', 'mouselike', 'quality', 'gus', 'might', 'smart'])\n",
      " list(['well', 'film', 'certainly', 'fair', 'amount', 'hype', 'from', 'buzz', 'toronto', 'film', 'festival', 'gushing', 'reviews', 'cbc', 'radio', 'a', 'refreshing', 'wacky', 'hilarious', 'indie', 'movie', 'apparently', 'id', 'looking', 'forward', 'monthsbr', 'br', 'how', 'i', 'put', 'simply', 'it', 'sucked', 'big', 'way', 'an', 'exercise', 'cinematic', 'lameness', 'makes', 'adam', 'sandler', 'look', 'like', 'comic', 'geniusbr', 'br', 'at', 'best', 'mildly', 'amusing', 'worst', 'coma', 'inducing', 'an', 'amerterish', 'script', 'badly', 'directed', 'shoddily', 'edited', 'ramshackle', 'messbr', 'br', 'and', 'music', 'db', 'louder', 'rest', 'sound', 'track', 'that', 'truly', 'odd', 'thing', 'itbr', 'br', 'if', 'want', 'see', 'genuinely', 'funny', 'bizarre', 'movie', 'involving', 'aliens', 'try', 'repoman', 'there', 'comparison'])\n",
      " ...\n",
      " list(['what', 'supposed', 'simple', 'generic', 'mystery', 'plot', 'involving', 'dead', 'philanthropist', 'is', 'fact', 'headache', 'inducing', 'tale', 'bunch', 'characters', 'the', 'big', 'actor', 'ginger', 'rogers', 'early', 'role', 'trying', 'find', 'murderer', 'among', 'small', 'cast', 'residents', 'posh', 'apartment', 'building', 'these', 'characters', 'range', 'utterly', 'stupid', 'downright', 'mean', 'as', 'cheap', 'low', 'budget', 'production', 'action', 'revolves', 'around', 'rogers', 'lead', 'man', 'some', 'guy', 'i', 'care', 'cause', 'really', 'sucked', 'talking', 'various', 'possibilities', 'solving', 'crime', 'constantly', 'cut', 'absurd', 'detective', 'head', 'butt', 'honestly', 'ive', 'never', 'worse', 'time', 'watching', 'old', 'brate', 'movie', 'type', 'ive', 'seen', 'real', 'headslappersbr', 'br', 'oh', 'butler', 'it', 'butler', 'but', 'pay', 'attention', 'guy', 'whos', 'closest', 'butler', 'there', 'ya', 'gobr', 'br', 'polarisdib'])\n",
      " list(['no', 'scenario', 'bad', 'actors', 'poor', 'melissa', 'gilbert', 'beurk', 'beurk', 'beurk', 'br', 'br', 'give', 'budget', 'make', 'this', 'in', 'belgium', 'make', 'ten', 'films', 'win', 'prices', 'cannes', 'thisbr', 'br', 'last', 'time', 'ive', 'seen', 'nullfilm', 'hypercube', 'but', 'scenario', 'betterbr', 'br', 'is', 'anyone', 'knows', 'director', 'graduate', 'schoolfilm', 'cop', 'br', 'br', 'the', 'better', 'things', 'film', 'word', 'endbr', 'br', 'why', 'authorize', 'sell', '', 'ç', 'expensive', 'br', 'br', 'ive', 'pay', 'ten', 'dollars', 'buy', 'thisbr', 'br', 'for', 'me', 'pay', 'big', 'mistake', 'millenniumbr', 'br', 'too', 'badbr', 'br', 'next', 'time', 'ill', 'break', 'arm', 'buy', 'type', 'sht'])\n",
      " list(['after', 'initial', 'shock', 'realizing', 'guts', 'mr', 'branagh', 'film', 'this', 'i', 'literally', 'shaking', 'excitement', 'epic', 'ahead', 'me', 'i', 'disappointed', 'so', 'true', 'shakespeare', 'yet', 'accessible', 'it', 'blew', 'mind', 'i', 'always', 'enjoy', 'seeing', 'rather', 'listening', 'to', 'branagh', 'made', 'wonderis', 'movie', 'dubbed', 'countries', 'that', 'would', 'like', 'painting', 'moustache', 'mona', 'lisa'])]\n"
     ]
    }
   ],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]\n",
    "\n",
    "del raw_data, raw_header # huge and no longer needed, get rid of it\n",
    "\n",
    "print(\"DEBUG::data_train::\")\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1020  980]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07333101  0.01511984 -0.03923827 ...  0.10405464 -0.08110102\n",
      "   0.25688022]\n",
      " [-0.08809602 -0.34617755 -0.01950213 ...  0.13445397 -0.05908645\n",
      "   0.02556805]\n",
      " [-0.0640822  -0.0522301  -0.03211598 ...  0.01951624  0.08673023\n",
      "   0.10480079]\n",
      " ...\n",
      " [-0.14213526 -0.171769   -0.00846663 ... -0.04127606  0.0761945\n",
      "   0.10384577]\n",
      " [ 0.22651275  0.03324785  0.1213126  ... -0.07920102  0.13148943\n",
      "   0.12953816]\n",
      " [ 0.02885111 -0.15764742 -0.00427123 ...  0.05920245 -0.02075605\n",
      "   0.00402516]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(data_train)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x/train_y list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[ 0.07333101  0.01511984 -0.03923827 ...  0.10405464 -0.08110102\n",
      "   0.25688022]\n",
      " [-0.08809602 -0.34617755 -0.01950213 ...  0.13445397 -0.05908645\n",
      "   0.02556805]\n",
      " [-0.0640822  -0.0522301  -0.03211598 ...  0.01951624  0.08673023\n",
      "   0.10480079]\n",
      " ...\n",
      " [ 0.0027437   0.01300152 -0.07666195 ... -0.02507846 -0.07781653\n",
      "   0.03148237]\n",
      " [-0.07563806 -0.18189028 -0.00728056 ... -0.05255522  0.12547016\n",
      "   0.2277591 ]\n",
      " [-0.11023627 -0.09230539 -0.06702375 ... -0.07511466 -0.00195359\n",
      "   0.17991148]]\n",
      "[0 1 0 1 0]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "data = EmbeddingVectors\n",
    "del EmbeddingVectors\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = header[idx:] \n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single IMDB Task Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "input_shape = (len(train_x[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 687us/step - loss: 0.5567 - accuracy: 0.7371 - val_loss: 0.4661 - val_accuracy: 0.7633\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 109us/step - loss: 0.4135 - accuracy: 0.8229 - val_loss: 0.4266 - val_accuracy: 0.7883\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 0.3616 - accuracy: 0.8543 - val_loss: 0.4243 - val_accuracy: 0.8167\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 109us/step - loss: 0.3338 - accuracy: 0.8771 - val_loss: 0.4739 - val_accuracy: 0.7800\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 0.3256 - accuracy: 0.8671 - val_loss: 0.5206 - val_accuracy: 0.7683\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 109us/step - loss: 0.3075 - accuracy: 0.8829 - val_loss: 0.4407 - val_accuracy: 0.8050\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 0.2989 - accuracy: 0.8679 - val_loss: 0.4518 - val_accuracy: 0.7950\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.2639 - accuracy: 0.8979 - val_loss: 0.4439 - val_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.2393 - accuracy: 0.9107 - val_loss: 0.5047 - val_accuracy: 0.7833\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.2502 - accuracy: 0.9064 - val_loss: 0.4920 - val_accuracy: 0.7883\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Email Task, Train Single Email Task Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Enron dataset and get a sense for the data by printing sample messages to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "filepath = \"../input/enron-email-dataset/emails.csv\"\n",
    "\n",
    "# Read the enron data into a pandas.DataFrame called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "print(emails.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate headers from the message bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "\n",
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from e-mails!\")\n",
    "    return messages\n",
    "\n",
    "bodies = extract_messages(emails)\n",
    "\n",
    "del emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Barbara/Jeff:\\n\\nIn the  TurboPark world there will be certain impacts on how power projects \\ninvolving physical capacity are structured and implemented.  I know some (but \\nnot all) of it.  I can say that it is fairly easy to bust the structure and \\nwind up with assets on the balance sheet, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>??? Dear Vince:\\n?\\n??? I have put together a list of  finance working papers (some of which I \\nbrought to the interview last wednesday)  which I have written since 1995 \\nmostly in support of my work but also (at least  initially) as a learning \\ntool. Several of them, however,?do contain  inn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Guys, what is the contingency plan to manage these outages in the event we \\ncan't manage this as effectively in future situations?\\n\\nRegards\\nDelainey\\n---------------------- Forwarded by David W Delainey/HOU/ECT on 06/29/2000 \\n12:25 PM ---------------------------\\n\\n\\nDavid W Delainey\\n06/29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lee,\\nThat works well.\\nCongratulations and thanks,\\nBill\\n\\n\\n\\n\\n\\nlfried@uclink4.berkeley.edu on 10/26/2000 10:06:39 PM\\nPlease respond to lfried@uclink4.berkeley.edu\\n\\n\\nTo: jeff.dasovich@enron.com, William Hederman/TCO/ColumbiaGas@COLUMBIAGAS,\\nAMosher@appanet.org, shapiro@haas.berkeley.ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GT,  How is it going?  Man, was that an ass-whipping or what that we took \\nfrom OU.  Looks like we salvaged some dignity with the CU game.  \\n\\nI was planning on coming in for the Missouri game.  I might have my brother \\nand another friend with me.  How are you set for space.  Those guys could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  Barbara/Jeff:\\n\\nIn the  TurboPark world there will be certain impacts on how power projects \\ninvolving physical capacity are structured and implemented.  I know some (but \\nnot all) of it.  I can say that it is fairly easy to bust the structure and \\nwind up with assets on the balance sheet, a...\n",
       "1  ??? Dear Vince:\\n?\\n??? I have put together a list of  finance working papers (some of which I \\nbrought to the interview last wednesday)  which I have written since 1995 \\nmostly in support of my work but also (at least  initially) as a learning \\ntool. Several of them, however,?do contain  inn...\n",
       "2  Guys, what is the contingency plan to manage these outages in the event we \\ncan't manage this as effectively in future situations?\\n\\nRegards\\nDelainey\\n---------------------- Forwarded by David W Delainey/HOU/ECT on 06/29/2000 \\n12:25 PM ---------------------------\\n\\n\\nDavid W Delainey\\n06/29...\n",
       "3  Lee,\\nThat works well.\\nCongratulations and thanks,\\nBill\\n\\n\\n\\n\\n\\nlfried@uclink4.berkeley.edu on 10/26/2000 10:06:39 PM\\nPlease respond to lfried@uclink4.berkeley.edu\\n\\n\\nTo: jeff.dasovich@enron.com, William Hederman/TCO/ColumbiaGas@COLUMBIAGAS,\\nAMosher@appanet.org, shapiro@haas.berkeley.ed...\n",
       "4  GT,  How is it going?  Man, was that an ass-whipping or what that we took \\nfrom OU.  Looks like we salvaged some dignity with the CU game.  \\n\\nI was planning on coming in for the Missouri game.  I might have my brother \\nand another friend with me.  How are you set for space.  Those guys could..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract random 10000 enron email bodies for building dataset\n",
    "import random\n",
    "bodies_df = pd.DataFrame(random.sample(bodies, 10000))\n",
    "\n",
    "del bodies # these are huge, no longer needed, get rid of them\n",
    "\n",
    "# expand default pandas display options to make emails more clearly visible when printed\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "bodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Preprocess Fraudulent \"419\" Email Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../input/fraudulent-email-corpus/fradulent_emails.txt\"\n",
    "with open(filepath, 'r',encoding=\"latin1\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on the code word `From r` appearing close to the beginning of each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3978 spam emails!\n"
     ]
    }
   ],
   "source": [
    "fraud_emails = data.split(\"From r\")\n",
    "\n",
    "del data\n",
    "\n",
    "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...\n",
       "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...\n",
       "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "4  Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"]))\n",
    "\n",
    "del fraud_emails\n",
    "\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
    "\n",
    "del fraud_bodies\n",
    "\n",
    "fraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "del bodies_df\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "del fraud_bodies_df\n",
    "\n",
    "raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data is:\n",
      "(2000,)\n",
      "Data is:\n",
      "[list(['frommr', 'james', 'tshabalalatel', 'south', 'africa', 'good', 'day', '', '', 'urgent', 'response', 'let', 'start', 'introducing', 'myself', 'i', 'mr', 'james', 'tshabalala', 'bank', 'manager', 'commercial', 'bank', 'south', 'africa', 'i', 'came', 'know', 'private', 'search', 'reliable', 'personcompany', 'handle', 'confidential', 'portfolio', 'supervision', 'during', 'visit', 'pretoria', 'one', 'associates', 'there', 'confidence', 'gave', 'contact', 'address', 'with', 'srong', 'hope', 'search', 'i', 'decided', 'put', 'proposal', 'the', 'portfolio', 'a', 'german', 'man', 'called', 'mr', 'wolfgang', 'schiniter', '', 'years', 'age', 'prosperous', 'farmer', 'deposited', 'us', 'sum', 'twenty', 'million', 'dollarsusmillion', 'his', 'wife', 'mrs', 'helga', 'schiniter', 'legitimate', 'beneficiary', 'account', 'unfortunately', 'killed', 'air', 'crash', 'involving', 'concord', 'af', 'gonesse', 'france', 'our', 'serch', 'german', 'embassy', 'find', 'next', 'kin', 'proved', 'fruitless', 'according', 'embassy', 'couple', 'offspring', 'the', 'firm'])\n",
      " list(['dear', 'friendcplease', 'bear', 'sending', 'letter', 'surprisinglye', 'my', 'name', 'mr', 'johnson', 'kalutec', 'young', 'man', 'originally', 'liberia', 'west', 'africaemy', 'sole', 'purpose', 'contacting', 'assist', 'selling', 'country', 'elsewhere', 'rough', 'uncutted', 'diamonds', 'weighs', 'c', 'caratscfrom', '', 'caratsckg', 'grosson', 'scalechvsqualityc', 'roughe', 'amount', '', '', '', 'million', 'dollarsthe', 'diamonds', 'deposited', 'late', 'father', 'holding', 'firm', 'waiting', 'order', 'transfered', 'i', 'next', 'of', 'kin', 'deposit', 'indicated', 'depositional', 'documentsei', 'presently', 'living', 'dakar', 'senegaleyou', 'reach', 'email', 'alternative', 'johnsonkalutehotmailecomwish', 'hear', 'soonebye', 'god', 'blesseyours', 'trulyjohnson', 'kalutee'])\n",
      " list(['cdearest', 'lovecmy', 'name', 'jimmy', 'johnson', 'sierraleonec', 'son', 'mre', 'ukpo', 'johnson', 'former', 'personal', 'assistance', 'present', 'minister', 'mines', 'natural', 'resources', 'incharge', 'diamond', 'production', 'sales', 'country', 'sierraleonee', 'unfortunately', 'months', 'ago', 'peace', 'pact', 'countryc', 'father', 'colleagues', 'killed', 'rebel', 'group', 'mre', 'foday', 'sankoh', 'inspection', 'visit', 'rich', 'diamond', 'mining', 'area', 'village', 'izumbac', 'rebels', 'laid', 'ambush', 'open', 'fire', 'convoy', 'killing', 'father', 'colleagues', 'way', 'diamond', 'mining', 'zonee', 'following', 'peace', 'pact', 'introduce', 'united', 'nation', 'keeping', 'force', 'months', 'backcmy', 'mother', 'deem', 'fit', 'say', 'right', 'time', 'us', 'leave', 'country', 'safety', 'europe', 'stable', 'economic', 'bloc', '', 'eemilliondollars', 'deposited', 'late', 'father', 'bank', 'dakarsenegal', 'name', 'next', 'kin', 'beneficiaryethe', 'deposit', 'certificate', 'funds', 'faxed', 'upon', 'requeste', 'i', 'contacted', 'based'])\n",
      " ...\n",
      " list(['you', 'received', 'message', 'someone', 'attempted', 'send', 'email', 'outside', 'enron', 'attachment', 'type', 'enron', 'allow', 'messaging', 'environment', 'your', 'email', 'quarantined', 'held', 'mailsweeper', 'serversender', 'daphnecoalltelnetdate', 'fri', '', 'dec', '', '', 'subject', 'traditional', 'wedding', 'cost', 'responsibilitiesattachment', 'type', 'scenariosincominginbound', 'url', 'catcher', 'a', 'filename', 'matching', 'file', 'mask', 'detected', 'traditional', 'wedding', 'cost', 'responsibilitiesurlif', 'intended', 'email', 'valid', 'businessrelated', 'content', 'believe', 'requires', 'enron', 'resources', 'retrieve', 'it', 'may', 'call', 'help', 'desk', 'ask', 'released', 'quarantine', 'delivered', 'email', 'inbox', 'your', 'message', 'scanned', 'checked', 'viruses', 'prior', 'requested', 'release', 'if', 'contains', 'virus', 'reason', 'suspect', 'malicious', 'code', 'deliverednorth', 'american', 'resolution', 'center', '', '', 'european', 'resolution', 'center', '', '', '', 'ees', 'help', 'desk', '', '', 'ets', 'solution', 'center', '', '', '', 'houston', '', '', '', 'omahaplease', 'do', 'not', 'reply', 'address', 'monitored', 'system', 'mailbox'])\n",
      " list(['set', 'stacy', 'walker', 'kelly', 'kimberly', 'luncheon', 'meeting'])\n",
      " list(['ran', 'aruba', 'sithe', 'books', 'order', 'become', 'familiar', 'risk', 'procedurestook', 'intra', 'central', 'bookhandled', 'oa', 'issues', 'deskrun', 'calc', 'sheet', 'deals', 'desktrained', 's', 'palmer', 'enron', 'north', 'america', 'corpfrom', 'jeffrey', 'c', 'gossett', '', '', 'amto', 'kam', 'keiserhouectect', 'victor', 'guggenheimhouectect', 'shawna', 'johnsoncorpenronenron', 'jackson', 'loganhouectect', 'darron', 'c', 'gironhouectect', 'monique', 'sanchezhouectect', 'dutch', 'quigleyhouectect', 'kyle', 'etterhouectect', 'burton', 'mcintyrehouectect', 'rahmaan', 'mwongozicorpenronenron', 'gabriel', 'monroyhouectect', 'robin', 'rodriguehouectectcc', 'subject', 'accomplishmentsplease', 'send', 'list', 'accomplishments', 'last', 'six', 'months', 'fridaythanksjg'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combined data is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data is:\")\n",
    "print(raw_data)\n",
    "\n",
    "# create corresponding labels\n",
    "Categories = ['spam','notspam']\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to convert these into numerical vectors!!\n",
    "\n",
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01536004 -0.12274602  0.06190309 ...  0.0405392  -0.11272036\n",
      "  -0.04488139]\n",
      " [-0.02831265 -0.08182404  0.20950457 ...  0.02200493 -0.01504305\n",
      "  -0.07850857]\n",
      " [-0.0064414  -0.02040358 -0.02121366 ...  0.0932903  -0.14455533\n",
      "  -0.09238692]\n",
      " ...\n",
      " [-0.3625687   0.07203355  0.11921321 ...  0.07324798 -0.10625815\n",
      "  -0.19990994]\n",
      " [-0.06652257 -0.25758487 -0.10346989 ... -0.4478352  -0.48571077\n",
      "   0.260466  ]\n",
      " [-0.18741988 -0.23663858  0.10094383 ... -0.03404405  0.02272647\n",
      "   0.06112502]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(raw_data)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x2/train_y2 (emails) list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[-1.1175805  -0.751737    0.55355763 ... -0.6607673   0.21230514\n",
      "  -0.30602062]\n",
      " [-0.03476024  0.01665223 -0.11075132 ...  0.05981117 -0.32611066\n",
      "  -0.16099696]\n",
      " [-0.21694274 -0.03368444  0.05471903 ... -0.24358185  0.02436963\n",
      "  -0.4627576 ]\n",
      " ...\n",
      " [-0.2266984  -0.0039735   0.25503    ...  0.14465979 -0.15364355\n",
      "  -0.1912163 ]\n",
      " [-0.01915166 -0.05129508 -0.06456647 ...  0.11918928 -0.21648325\n",
      "  -0.02365398]\n",
      " [-1.1175805  -0.751737    0.55355763 ... -0.6607673   0.21230514\n",
      "  -0.30602062]]\n",
      "[1 0 0 1 0]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p,:]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "data, header = unison_shuffle_data(EmbeddingVectors, header)\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x2 = data[:idx,:]\n",
    "train_y2 = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x2 = data[idx:,:]\n",
    "test_y2 = header[idx:] \n",
    "\n",
    "print(\"train_x2/train_y2 (emails) list details, to make sure it is of the right form:\")\n",
    "print(len(train_x2))\n",
    "print(train_x2)\n",
    "print(train_y2[:5])\n",
    "print(len(train_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \"just email\" single-task shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 0s 275us/step - loss: 0.2140 - accuracy: 0.9379 - val_loss: 0.0689 - val_accuracy: 0.9833\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.0604 - val_accuracy: 0.9783\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0531 - val_accuracy: 0.9867\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.0532 - val_accuracy: 0.9850\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 111us/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.0559 - val_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 129us/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.0631 - val_accuracy: 0.9833\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0586 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.0604 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.0054 - accuracy: 0.9979 - val_loss: 0.0627 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 0.0051 - accuracy: 0.9979 - val_loss: 0.0606 - val_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x2, train_y2, validation_data=(test_x2, test_y2), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Double-Task\" Email and IMDB System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "input1_shape = (len(train_x[0]),)\n",
    "input2_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors1 = Input(shape=input1_shape)\n",
    "sent2vec_vectors2 = Input(shape=input2_shape)\n",
    "combined = concatenate([sent2vec_vectors1,sent2vec_vectors2])\n",
    "dense1 = Dense(512, activation='relu')(combined)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "output1 = Dense(1, activation='sigmoid',name='classification1')(dense1)\n",
    "output2 = Dense(1, activation='sigmoid',name='classification2')(dense1)\n",
    "model = Model(inputs=[sent2vec_vectors1,sent2vec_vectors2], outputs=[output1,output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 0.8823 - classification1_loss: 0.6246 - classification2_loss: 0.2558 - classification1_accuracy: 0.6879 - classification2_accuracy: 0.9179 - val_loss: 0.6576 - val_classification1_loss: 0.5611 - val_classification2_loss: 0.0954 - val_classification1_accuracy: 0.7217 - val_classification2_accuracy: 0.9767\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 175us/step - loss: 0.5125 - classification1_loss: 0.4571 - classification2_loss: 0.0559 - classification1_accuracy: 0.8021 - classification2_accuracy: 0.9907 - val_loss: 0.5689 - val_classification1_loss: 0.4974 - val_classification2_loss: 0.0702 - val_classification1_accuracy: 0.7267 - val_classification2_accuracy: 0.9783\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 178us/step - loss: 0.3954 - classification1_loss: 0.3649 - classification2_loss: 0.0309 - classification1_accuracy: 0.8514 - classification2_accuracy: 0.9957 - val_loss: 0.5225 - val_classification1_loss: 0.4568 - val_classification2_loss: 0.0642 - val_classification1_accuracy: 0.7867 - val_classification2_accuracy: 0.9817\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 174us/step - loss: 0.3359 - classification1_loss: 0.3176 - classification2_loss: 0.0190 - classification1_accuracy: 0.8843 - classification2_accuracy: 0.9971 - val_loss: 0.5590 - val_classification1_loss: 0.4931 - val_classification2_loss: 0.0645 - val_classification1_accuracy: 0.7650 - val_classification2_accuracy: 0.9783\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 171us/step - loss: 0.2956 - classification1_loss: 0.2814 - classification2_loss: 0.0140 - classification1_accuracy: 0.8836 - classification2_accuracy: 0.9993 - val_loss: 0.5742 - val_classification1_loss: 0.5064 - val_classification2_loss: 0.0660 - val_classification1_accuracy: 0.7633 - val_classification2_accuracy: 0.9783\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 174us/step - loss: 0.2668 - classification1_loss: 0.2563 - classification2_loss: 0.0102 - classification1_accuracy: 0.8921 - classification2_accuracy: 0.9993 - val_loss: 0.5554 - val_classification1_loss: 0.4936 - val_classification2_loss: 0.0596 - val_classification1_accuracy: 0.7817 - val_classification2_accuracy: 0.9817\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 316us/step - loss: 0.2500 - classification1_loss: 0.2430 - classification2_loss: 0.0072 - classification1_accuracy: 0.9036 - classification2_accuracy: 0.9993 - val_loss: 0.5776 - val_classification1_loss: 0.5127 - val_classification2_loss: 0.0624 - val_classification1_accuracy: 0.7667 - val_classification2_accuracy: 0.9817\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 232us/step - loss: 0.2203 - classification1_loss: 0.2159 - classification2_loss: 0.0061 - classification1_accuracy: 0.9243 - classification2_accuracy: 1.0000 - val_loss: 0.6403 - val_classification1_loss: 0.5757 - val_classification2_loss: 0.0609 - val_classification1_accuracy: 0.7650 - val_classification2_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 171us/step - loss: 0.1954 - classification1_loss: 0.1903 - classification2_loss: 0.0050 - classification1_accuracy: 0.9264 - classification2_accuracy: 1.0000 - val_loss: 0.6908 - val_classification1_loss: 0.6265 - val_classification2_loss: 0.0612 - val_classification1_accuracy: 0.7300 - val_classification2_accuracy: 0.9833\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 173us/step - loss: 0.1910 - classification1_loss: 0.1867 - classification2_loss: 0.0043 - classification1_accuracy: 0.9279 - classification2_accuracy: 1.0000 - val_loss: 0.6591 - val_classification1_loss: 0.5894 - val_classification2_loss: 0.0663 - val_classification1_accuracy: 0.7683 - val_classification2_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss={'classification1': 'binary_crossentropy', \n",
    "                    'classification2': 'binary_crossentropy'},\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit([train_x,train_x2],[train_y,train_y2],\n",
    "                    validation_data=([test_x,test_x2],[test_y,test_y2]),\n",
    "                                     batch_size=32, nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "#create_download_link(filename='file.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf aclImdb\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
