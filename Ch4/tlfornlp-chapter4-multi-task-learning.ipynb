{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/epfml/sent2vec\r\n",
      "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-k78wm5x6\r\n",
      "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-k78wm5x6\r\n",
      "Building wheels for collected packages: sent2vec\r\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1139399 sha256=5564d8fd6978667350f5a1ac2beea444aa5bab53295e5995f3919d49deb93ebd\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ly9a2fon/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\r\n",
      "Successfully built sent2vec\r\n",
      "Installing collected packages: sent2vec\r\n",
      "Successfully installed sent2vec-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "# install sent2vec\n",
    "!pip install git+https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n",
    "\n",
    "Latest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download IMDB Movie Review Dataset\n",
    "Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "## Read-in the reviews and print some basic descriptions of them\n",
    "\n",
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization, Stop-word and Punctuation Removal Functions\n",
    "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use regular expressions to remove unnecessary characters**\n",
    "\n",
    "Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower() # make all characters lower case\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop-word removal**\n",
    "\n",
    "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "# print(stopwords) # see default stopwords\n",
    "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
    "# of a sentence\n",
    "# stopwords.remove(\"no\")\n",
    "# stopwords.remove(\"nor\")\n",
    "# stopwords.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Embedding Vectors\n",
    "The following functions are used to extract sent2vec embedding vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sent2vec embedding took 41 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sent2vec\n",
    "\n",
    "s2v_model = sent2vec.Sent2vecModel()\n",
    "start=time.time()\n",
    "s2v_model.load_model('../input/sent2vec/wiki_unigrams.bin')\n",
    "end = time.time()\n",
    "print(\"Loading the sent2vec embedding took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_embedding_vectors(data):\n",
    "    out = None\n",
    "    for item in data:\n",
    "        vec = s2v_model.embed_sentence(\" \".join(item))\n",
    "        if vec is not None:\n",
    "            if out is not None:\n",
    "                out = np.concatenate((out,vec),axis=0)\n",
    "            else:\n",
    "                out = vec                                            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together To Assemble Dataset\n",
    "Now, putting all the preprocessing steps together we assemble our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "# load data in appropriate form\n",
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    \n",
    "    return data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::data_train::\n",
      "[list(['this', 'documentary', 'aired', 'rte', 'bbc', 'last', 'number', 'months', 'having', 'seen', 'twice', 'i', 'would', 'recommend', 'anyone', 'interest', 'media', 'documentary', 'film', 'makingbr', 'br', 'initially', 'documentary', 'meant', 'detail', 'political', 'life', 'venezuelan', 'president', 'hugo', 'chavez', 'the', 'irish', 'crew', 'set', 'intentions', 'what', 'happens', 'get', 'venezuela', 'startling', 'witness', 'first', 'hand', 'attempted', 'overthrow', 'rebel', 'factions', 'particularly', 'oil', 'concerns', 'venezuela', 'chavez', 'government', 'what', 'audience', 'witness', 'media', 'manipulates', 'situation', 'effect', 'backs', 'overthrow', 'chavez', 'distorting', 'events', 'transpire', 'coup', 'heightensbr', 'br', 'it', 'really', 'excellent', 'documentary', 'remarkable', 'piece', 'work', 'couple', 'novice', 'filmmakersbr', 'br', ''])\n",
      " list(['simply', 'one', 'funiest', 'movies', 'ive', 'ever', 'seen', 'its', 'parody', 'crimelife', 'parody', 'everything', 'represents', 'chicago', '', 'there', 'realy', 'need', 'underestimate', 'movie', 'rating', '', 'its', 'opinion', 'mass', 'mass', 'hypnotized', 'who', 'decide', 'watch', '', 'regret', 'who', 'decide', 'watch', '', 'regret', 'more'])\n",
      " list(['this', 'movies', 'origins', 'mystery', 'me', 'i', 'know', 'much', 'imdb', 'i', 'rented', 'it', 'i', 'assume', 'starship', 'troopers', 'killshot', 'one', 'countless', 'unaired', 'pilots', 'never', 'made', 'network', 'cable', 'otherwise', 'the', 'new', 'title', 'kill', 'shot', 'comically', 'thrown', 'opening', 'sequence', 'first', 'many', 'quick', 'clues', 'ever', 'intended', 'cinema', 'the', 'quick', 'cuts', 'cheesy', 'melrose', 'place', 'music', 'short', 'second', 'closeup', 'candid', 'shots', 'main', 'actors', 'let', 'know', 'forbr', 'br', 'and', 'i', 'mind', 'all', 'i', 'rented', 'movie', 'seeing', 'repackaging', 'puts', 'casper', 'van', 'dien', 'denise', 'richards', 'cover', 'front', 'volleyball', 'net', 'thinking', 'would', 'funny', 'see', 'movie', 'besides', 'scifi', 'travesty', 'starship', 'troopers', 'an', 'excellent', 'book', 'opinion', 'hot', 'movie', '', 'thats', 'another', 'review', 'after', 'looking', 'imdb', 'roommate', 'i', 'surmised', 'pilot', 'dragged', 'apparent', 'success', 'troopers', 'richards', 'career', 'see', 'bondgirl', 'wild', 'things', 'references', 'here', 'they', 'threw', 'sex'])\n",
      " ...\n",
      " list(['this', 'movie', 'billed', 'comedy', 'mystery', 'it', 'fails', 'badly', 'both', 'the', 'mystery', 'would', 'anybody', 'make', 'poorly', 'constructed', 'movie', 'the', 'comedy', 'laugh', 'i', 'got', 'i', 'saw', 'high', 'readers', 'ranked', 'it', 'could', 'two', 'movies', 'name', 'the', 'movie', 'i', 'saw', 'starred', 'girl', 'pretty', 'blue', 'eyes', 'plot', 'therebr', 'br', ''])\n",
      " list(['first', 'off', 'im', 'huge', 'fan', 's', 'movies', 'jennifer', 'conelly', 'well', 'so', 'yesterday', 'i', 'wandered', 'local', 'used', 'bookmovie', 'store', 'found', 'vhs', 'copy', 'i', 'read', 'back', 'sounded', 'good', '', 'good', 'deal', 'so', 'i', 'took', 'home', 'popped', 'vcr', 'what', 'sweet', 'movie', 'at', 'age', 'now', 'i', 'relate', 'movies', 'like', 'about', 'last', 'night', 'st', 'elmos', 'fire', 'still', 'i', 'remember', 'like', '', 'love', 'older', 'guy', 'etc', 'we', 'little', 'crushes', 'were', 'younger', 'and', 'work', 'out', 'were', 'heartbroken', 'think', 'well', 'never', 'get', 'it', 'but', 'course', 'do', 'many', 'times', 'its', 'sort', 'sweet', 'quality', 'i', 'really', 'got', 'movie', 'the', 'feeling', 'oooh', 'i', 'remember', 'something', 'like', 'happened', 'me', 'it', 'the', 'characters', 'interesting', 'welldeveloped', 'i', 'recommend', 'anyone', 'likes', 's', 'movies', 'teen', 'films', 'particular', 'anyone', 'wants', 'go', 'back', 'remember', 'simpler', 'time', 'lives'])\n",
      " list(['lillian', 'hellman', 'one', 'americas', 'famous', 'women', 'playwrights', 'woman', 'mission', 'her', 'leftist', 'views', 'well', 'regarded', 'time', 'country', 'in', 'memoir', 'recounts', 'trip', 'then', 'soviet', 'union', 'intrigued', 'called', 'successes', 'achieved', 'system', 'watch', 'rhine', 'must', 'come', 'result', 'years', 'the', 'left', 'wing', 'america', 'world', 'issue', 'rise', 'fascism', 'europe', 'japan', 'wellbr', 'br', 'watch', 'rhine', 'play', 'produced', 'broadway', 'eight', 'months', 'pearl', 'harbor', 'attack', 'japanese', 'in', 'ms', 'hellman', 'heralding', 'americas', 'entrance', 'world', 'war', 'ii', 'the', 'adaptation', 'credited', 'ms', 'hellman', 'dashiell', 'hammett', 'long', 'time', 'companion', 'as', 'directed', 'screen', 'herman', 'shumlin', 'film', 'well', 'received', 'premiered', 'br', 'br', 'we', 'introduced', 'muller', 'family', 'film', 'opens', 'they', 'crossing', 'border', 'united', 'states', 'mexico', 'they', 'continue', 'toward', 'mrs', 'mullers', 'home', 'washington', 'dc', 'mother', 'fanny', 'farrelly', 'minor'])]\n"
     ]
    }
   ],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]\n",
    "\n",
    "del raw_data, raw_header # huge and no longer needed, get rid of it\n",
    "\n",
    "print(\"DEBUG::data_train::\")\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1008  992]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0102319  -0.25911504  0.32173342 ...  0.0382418  -0.28723043\n",
      "   0.16901135]\n",
      " [ 0.01625078 -0.266056    0.11902541 ... -0.15257029 -0.06450036\n",
      "   0.0200811 ]\n",
      " [ 0.10394059 -0.0421819   0.00287194 ...  0.00950469 -0.09494772\n",
      "   0.0721978 ]\n",
      " ...\n",
      " [-0.00285244 -0.11204793 -0.04106293 ...  0.07098608  0.03362228\n",
      "   0.07496912]\n",
      " [ 0.03992646 -0.06495458 -0.06775349 ...  0.13719364 -0.21264713\n",
      "   0.1838117 ]\n",
      " [-0.22419588 -0.12976232  0.02702025 ...  0.03708775 -0.04663878\n",
      "   0.0180573 ]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(data_train)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x/train_y list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[ 0.0102319  -0.25911504  0.32173342 ...  0.0382418  -0.28723043\n",
      "   0.16901135]\n",
      " [ 0.01625078 -0.266056    0.11902541 ... -0.15257029 -0.06450036\n",
      "   0.0200811 ]\n",
      " [ 0.10394059 -0.0421819   0.00287194 ...  0.00950469 -0.09494772\n",
      "   0.0721978 ]\n",
      " ...\n",
      " [-0.09276448 -0.21992254 -0.00569641 ... -0.14773265 -0.15770121\n",
      "   0.15325668]\n",
      " [-0.07607748 -0.06993847 -0.11272059 ... -0.11972501 -0.01401911\n",
      "  -0.03252426]\n",
      " [-0.00111649 -0.07526951  0.07201906 ...  0.12033443  0.00771973\n",
      "   0.18872222]]\n",
      "[1 1 1 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "data = EmbeddingVectors\n",
    "del EmbeddingVectors\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = header[idx:] \n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single IMDB Task Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "input_shape = (len(train_x[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 902us/step - loss: 0.5830 - accuracy: 0.6871 - val_loss: 0.5237 - val_accuracy: 0.7317\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 0.4317 - accuracy: 0.8086 - val_loss: 0.4781 - val_accuracy: 0.7783\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 0.3615 - accuracy: 0.8486 - val_loss: 0.4236 - val_accuracy: 0.8033\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 0.3288 - accuracy: 0.8614 - val_loss: 0.4145 - val_accuracy: 0.8050\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 0.2958 - accuracy: 0.8729 - val_loss: 0.4283 - val_accuracy: 0.8050\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.2745 - accuracy: 0.8850 - val_loss: 0.4317 - val_accuracy: 0.8100\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 0.2547 - accuracy: 0.8921 - val_loss: 0.4465 - val_accuracy: 0.7967\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 107us/step - loss: 0.2406 - accuracy: 0.9036 - val_loss: 0.4607 - val_accuracy: 0.8100\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 107us/step - loss: 0.2224 - accuracy: 0.9093 - val_loss: 0.4736 - val_accuracy: 0.7950\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 107us/step - loss: 0.2009 - accuracy: 0.9271 - val_loss: 0.4849 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Email Task, Train Single Email Task Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Enron dataset and get a sense for the data by printing sample messages to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "filepath = \"../input/enron-email-dataset/emails.csv\"\n",
    "\n",
    "# Read the enron data into a pandas.DataFrame called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "print(emails.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate headers from the message bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "\n",
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from e-mails!\")\n",
    "    return messages\n",
    "\n",
    "bodies = extract_messages(emails)\n",
    "\n",
    "del emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Not much has happened with the federal filings since we submitted them (SEC \\napplication was filed on February 4, FERC application was filed on March 3).  \\nAnn has been keeping everyone updated on the DOJ review of the HSR filing.  \\n\\nWith regard to the FERC filing, FERC separated the section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>---------------------- Forwarded by Robin Rodrigue/HOU/ECT on 12/04/2000 \\n04:22 PM ---------------------------\\n\\n\\nChris Abel\\n08/11/2000 11:38 AM\\nTo: Michael Benien/Corp/Enron@ENRON, Daniel Falcone/Corp/Enron@ENRON, Michael \\nE Moscoso/HOU/ECT@ECT, Gabriel Monroy/HOU/ECT@ECT, Robin \\nRodrigu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>What a mess!  \\n\\n\\n\\n\\nsuzanneadams@att.net on 04/09/2001 06:50:22 PM\\nTo: Kay.Mann@enron.com\\ncc:  \\n\\nSubject: Re: Doctor's Report\\n\\nYou know I forgot all about the conference, but the doc \\npicked the day not me.  Anyhow you look at it though, \\nthe timing is good.  Cool!  I'm going to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>----- Forwarded by Cindy Derecskey/Corp/Enron on 12/08/2000 09:45 AM -----\\n\\n\\tEmma Facy@ECT\\n\\t12/08/2000 03:41 AM\\n\\t\\t \\n\\t\\t To: Jackie Gentle/LON/ECT@ECT, Mark Pickering/LON/ECT@ECT, John \\nSherriff/LON/ECT@ECT, Karen Denne/Corp/Enron@ENRON, Cindy \\nDerecskey/Corp/Enron@Enron, Ann M Schmid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Kay,\\n\\nI am not sure what we decided on Friday.  Are we going to send another (more \\nfriendly) letter to the Westinghouse people evidencing a willingness to \\ndiscuss the draw issue?  Please let me know at your convenience.\\n\\nStuart\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  Not much has happened with the federal filings since we submitted them (SEC \\napplication was filed on February 4, FERC application was filed on March 3).  \\nAnn has been keeping everyone updated on the DOJ review of the HSR filing.  \\n\\nWith regard to the FERC filing, FERC separated the section...\n",
       "1  ---------------------- Forwarded by Robin Rodrigue/HOU/ECT on 12/04/2000 \\n04:22 PM ---------------------------\\n\\n\\nChris Abel\\n08/11/2000 11:38 AM\\nTo: Michael Benien/Corp/Enron@ENRON, Daniel Falcone/Corp/Enron@ENRON, Michael \\nE Moscoso/HOU/ECT@ECT, Gabriel Monroy/HOU/ECT@ECT, Robin \\nRodrigu...\n",
       "2  What a mess!  \\n\\n\\n\\n\\nsuzanneadams@att.net on 04/09/2001 06:50:22 PM\\nTo: Kay.Mann@enron.com\\ncc:  \\n\\nSubject: Re: Doctor's Report\\n\\nYou know I forgot all about the conference, but the doc \\npicked the day not me.  Anyhow you look at it though, \\nthe timing is good.  Cool!  I'm going to have...\n",
       "3  ----- Forwarded by Cindy Derecskey/Corp/Enron on 12/08/2000 09:45 AM -----\\n\\n\\tEmma Facy@ECT\\n\\t12/08/2000 03:41 AM\\n\\t\\t \\n\\t\\t To: Jackie Gentle/LON/ECT@ECT, Mark Pickering/LON/ECT@ECT, John \\nSherriff/LON/ECT@ECT, Karen Denne/Corp/Enron@ENRON, Cindy \\nDerecskey/Corp/Enron@Enron, Ann M Schmid...\n",
       "4                                                                Kay,\\n\\nI am not sure what we decided on Friday.  Are we going to send another (more \\nfriendly) letter to the Westinghouse people evidencing a willingness to \\ndiscuss the draw issue?  Please let me know at your convenience.\\n\\nStuart\\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract random 10000 enron email bodies for building dataset\n",
    "import random\n",
    "bodies_df = pd.DataFrame(random.sample(bodies, 10000))\n",
    "\n",
    "del bodies # these are huge, no longer needed, get rid of them\n",
    "\n",
    "# expand default pandas display options to make emails more clearly visible when printed\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "bodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Preprocess Fraudulent \"419\" Email Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../input/fraudulent-email-corpus/fradulent_emails.txt\"\n",
    "with open(filepath, 'r',encoding=\"latin1\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on the code word `From r` appearing close to the beginning of each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3978 spam emails!\n"
     ]
    }
   ],
   "source": [
    "fraud_emails = data.split(\"From r\")\n",
    "\n",
    "del data\n",
    "\n",
    "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...\n",
       "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...\n",
       "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "4  Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"]))\n",
    "\n",
    "del fraud_emails\n",
    "\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
    "\n",
    "del fraud_bodies\n",
    "\n",
    "fraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "del bodies_df\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "del fraud_bodies_df\n",
    "\n",
    "raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data is:\n",
      "(2000,)\n",
      "Data is:\n",
      "[list(['', 'netherlotto', 'corporation', 'netherlotto', 'corporationc', '', 'c', 'nl', 'db', 'amsterdamc', 'the', 'netherlandse', '', 'froma', 'the', 'desk', 'of', 'the', 'director', 'promotionsc', 'international', 'promotionsfprize', 'award', 'departmentc', 'refa', 'eguyis', '', '', '', 'we', 'pleased', 'inform', 'announcement', 'thecth', 'septembere', 'c', 'winners', 'netherlotto', 'corpef', 'international', 'programs', 'held', 'th', 'june', 'e', 'you', 'f', 'companyc', 'attached', 'ticket', 'number', 'c', 'with', 'serial', 'number', 'a', 'drew', 'lucky', 'numbers', 'cand', 'consequently', 'category', 'ce', 'you', 'therefore', 'approved', 'a', 'lump', 'sum', 'pay', 'uscce', 'cash', 'credited', 'file', 'ref', 'noe', 'eguyis', 'this', 'total', 'prize', 'money', 'us', 'cce', 'shared', 'among', 'nineteen', 'international', 'winners', 'the', 'category', 'ce', 'all', 'participants', 'selected', 'througha', 'computer', 'ballot', 'system', 'drawn', 'c', 'names', 'australiac', 'africacnew', 'zealandc', '', 'americac', 'asiac', 'europecusa', 'north', 'america', 'part', 'international'])\n",
      " list(['emailmessagemessage', 'object', 'xfafeab', 'emailmessagemessage', 'object', 'xff', 'emailmessagemessage', 'object', 'xffcbcf'])\n",
      " list(['box', '', 'port', 'shepstonechaka', 'south', 'africaurgent', 'and', 'confidentialre', 'transfer', 'of', '', '', 'usdone', 'hundred', 'and', 'twenty', 'six', 'million', 'dollarsdear', 'sirmadamwe', 'want', 'transfer', 'overseas', '', '', 'usdone', 'hundred', 'twenty', 'six', 'million', 'united', 'statesdollars', 'bank', 'africa', 'you', 'serve', 'toreceive', 'money', 'long', 'remain', 'honestto', 'till', 'end', 'important', 'businesstrusting', 'believing', 'god', 'willnever', 'let', 'either', 'futurei', 'drmark', 'chuks', 'auditor', 'general', 'bank', 'inafrica', 'course', 'auditing', 'i', 'discovereda', 'floating', 'fund', 'account', 'opened', 'bank', 'in', 'since', '', 'nobody', 'operated', 'thisaccount', 'again', 'going', 'old', 'files', 'inthe', 'records', 'i', 'discovered', 'owner', 'accountdied', 'without', 'heir', 'hence', 'money', 'floating', 'andif', 'i', 'remit', 'money', 'urgently', 'beforfeited', 'nothing', 'owner', 'account', 'ismr', 'frankline', 'sullivan', 'foreigner', 'sailor', 'andhe', 'died', 'since', '', 'person', 'knows', 'aboutthis', 'account', 'thing', 'concerning', 'it', 'accounthas', 'beneficiary', 'investigation', 'provedto', 'well', 'frankline', 'sullivan', 'deathwas'])\n",
      " ...\n",
      " list(['httpwwwconsultrcicomenergy', 'exchanges', 'online', '', 'scottsdale', '', 'december', 'bb', 'ecommerce', 'revolutionized', 'commodity', 'trading', 'ad', 'process', 'andprocurement', 'within', 'energy', 'industry', 'with', 'heavyweight', 'keynotes', 'indepthpanel', 'discussions', 'first', 'event', 'bring', 'major', 'energy', 'companiesnet', 'markets', 'venture', 'capitalists', 'regulatory', 'bodies', 'investment', 'banks', 'andanalysts', 'together', 'thrash', 'business', 'usual', 'businessonlinefor', 'information', 'please', 'visit', 'wwweyeforenergycomxonlinescientech', 'issuealert', 'november', '', 'the', 'new', 'power', 'company', 'going', 'green', 'while', 'stock', 'price', 'dropsby', 'will', 'mcnamara', 'director', 'electric', 'industry', 'analysisthe', 'new', 'power', 'company', 'tnpc', 'nyse', 'npw', 'bills', 'firstnational', 'residential', 'small', 'business', 'energy', 'provider', 'announced', 'thatit', 'signed', 'letter', 'intent', 'community', 'energy', 'inc', 'providewindgenerated', 'energy', 'customers', 'philadelphia', 'area', 'under', 'theterms', 'agreement', 'tnpc', 'market', 'wind', 'power', 'supplied', 'communityenergy', 'site', 'development', 'mountains', 'northeast', 'pennsylvaniathe', 'new', 'wind', 'turbines', 'scheduled', 'come', 'online', 'end', 'analysis', 'this', 'interesting', 'development', 'especially', 'considering', 'thattnpc', 'still', 'embroiled', 'battle', 'green', 'mountain', 'significantnumber', 'customers', 'tnpc', 'recently', 'secured', 'bidding', 'process', 'frompeco', 'energy', 'this', 'could', 'attempt', 'part', 'tnpc'])\n",
      " list(['please', 'let', 'know', 'dan', 'attend', '', 'rika', 'original', 'messagefrom', 'dan', 'salter', 'mailtodsalterhgpinccomsent', 'monday', 'october', '', '', '', 'amto', 'imai', 'rikasubject', 'meetingrikaon', 'dec', '', '', 'conference', 'energy', 'policy', 'regulation', 'key', 'speakers', 'includerichard', 'meserve', '', 'chairman', 'nuclear', 'regulatory', 'commission', 'nrcpat', 'wood', '', 'chairman', 'federal', 'energy', 'regulatoy', 'commission', 'fercfrancis', 'blake', '', 'dep', 'secretary', 'energy', 'us', 'doejames', 'connaughton', '', 'chairman', 'white', 'house', 'council', 'environmental', 'qualityjeff', 'bingaman', '', 'senator', 'chairman', 'senate', 'committee', 'energy', 'natural', 'resourcescarl', 'wood', '', 'commissioner', 'california', 'public', 'utilities', 'commissionand', 'otherstopics', 'includenew', 'nuclear', 'power', 'generation', 'usbreaking', 'nuclear', 'waste', 'policy', 'logjamferc', 'policy', 'restructuring', 'wholesale', 'marketsferc', 'policy', 'creating', 'four', 'megartos', 'plus', 'ercotnational', 'energy', 'environmental', 'policy', 'bush', 'administration', 'perspective', '', 'congressional', 'perspectivethe', 'future', 'state', 'restructuringand', 'othersas', 'see', 'major', 'conference', 'lot', 'key', 'issues', 'lot', 'key', 'individuals', 'delivering', 'views', 'leading', 'discussions', 'i', 'think', 'would', 'beneficial', 'us', 'attend', 'again', 'since', 'conference', 'rather', 'nrc', 'meeting', 'i', 'would', 'suggest', 'tradeoff', 'us', 'paying', 'flight', 'hotel', 'expenses', 'time'])\n",
      " list(['that', 'i', 'love', 'anything', 'three', 'four', 'times', 'i', 'love', 'much', 'thanks', 'nookie', 'morning', 'you', 'know', 'health', 'books', 'say', 'really', 'good', 'sexorgasms', 'twice', 'week', 'coldflu', 'season', 'helps', 'strengthen', 'immune', 'system', 'this', 'sounds', 'dorky', 'maybe', 'set', 'lovemaking', 'dates', 'everyweek', 'stick', 'it', 'love', 'kisses'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combined data is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data is:\")\n",
    "print(raw_data)\n",
    "\n",
    "# create corresponding labels\n",
    "Categories = ['spam','notspam']\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to convert these into numerical vectors!!\n",
    "\n",
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03317764  0.07362798  0.19308645 ... -0.04788522 -0.01147885\n",
      "  -0.00251804]\n",
      " [-1.1175807  -0.751737    0.55355763 ... -0.6607673   0.21230516\n",
      "  -0.30602062]\n",
      " [-0.0673231   0.01148732 -0.03144034 ... -0.0213981  -0.03132029\n",
      "   0.01879456]\n",
      " ...\n",
      " [-0.13396886 -0.03065061  0.11255352 ... -0.1317871  -0.23691481\n",
      "   0.02130968]\n",
      " [-0.07162081  0.07984769 -0.12441923 ... -0.3069535  -0.2733425\n",
      "  -0.00550599]\n",
      " [-0.20956394  0.1029416  -0.01892696 ...  0.11135242 -0.15208471\n",
      "   0.18950589]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(raw_data)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x2/train_y2 (emails) list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[-0.07138915 -0.02298339 -0.01392924 ...  0.01740107 -0.13331647\n",
      "  -0.06678196]\n",
      " [ 0.12193154  0.2283633   0.08305462 ...  0.11863346 -0.25589672\n",
      "  -0.09505108]\n",
      " [-1.1175805  -0.751737    0.55355763 ... -0.6607673   0.21230514\n",
      "  -0.30602062]\n",
      " ...\n",
      " [-0.05288233 -0.09583424 -0.09330093 ...  0.18604138 -0.22734706\n",
      "  -0.13026707]\n",
      " [-0.15514517 -0.01878629  0.05485336 ...  0.06092846 -0.19817528\n",
      "   0.17559372]\n",
      " [ 0.06505737 -0.21145517 -0.19747108 ... -0.02044362 -0.14387816\n",
      "   0.0815329 ]]\n",
      "[1 0 1 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p,:]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "data, header = unison_shuffle_data(EmbeddingVectors, header)\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x2 = data[:idx,:]\n",
    "train_y2 = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x2 = data[idx:,:]\n",
    "test_y2 = header[idx:] \n",
    "\n",
    "print(\"train_x2/train_y2 (emails) list details, to make sure it is of the right form:\")\n",
    "print(len(train_x2))\n",
    "print(train_x2)\n",
    "print(train_y2[:5])\n",
    "print(len(train_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \"just email\" single-task shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 0s 284us/step - loss: 0.1950 - accuracy: 0.9579 - val_loss: 0.0426 - val_accuracy: 0.9933\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.0346 - accuracy: 0.9936 - val_loss: 0.0257 - val_accuracy: 0.9967\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 116us/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.0193 - val_accuracy: 0.9967\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9967\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 116us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9967\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x2, train_y2, validation_data=(test_x2, test_y2), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Double-Task\" Email and IMDB System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "input1_shape = (len(train_x[0]),)\n",
    "input2_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors1 = Input(shape=input1_shape)\n",
    "sent2vec_vectors2 = Input(shape=input2_shape)\n",
    "combined = concatenate([sent2vec_vectors1,sent2vec_vectors2])\n",
    "dense1 = Dense(512, activation='relu')(combined)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "output1 = Dense(1, activation='sigmoid',name='classification1')(dense1)\n",
    "output2 = Dense(1, activation='sigmoid',name='classification2')(dense1)\n",
    "model = Model(inputs=[sent2vec_vectors1,sent2vec_vectors2], outputs=[output1,output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 739us/step - loss: 0.9048 - classification1_loss: 0.6499 - classification2_loss: 0.2543 - classification1_accuracy: 0.6243 - classification2_accuracy: 0.9079 - val_loss: 0.7043 - val_classification1_loss: 0.6368 - val_classification2_loss: 0.0672 - val_classification1_accuracy: 0.6317 - val_classification2_accuracy: 0.9933\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 320us/step - loss: 0.5264 - classification1_loss: 0.4693 - classification2_loss: 0.0568 - classification1_accuracy: 0.7779 - classification2_accuracy: 0.9879 - val_loss: 0.5506 - val_classification1_loss: 0.5152 - val_classification2_loss: 0.0369 - val_classification1_accuracy: 0.7683 - val_classification2_accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 272us/step - loss: 0.4123 - classification1_loss: 0.3798 - classification2_loss: 0.0331 - classification1_accuracy: 0.8357 - classification2_accuracy: 0.9943 - val_loss: 0.5283 - val_classification1_loss: 0.4951 - val_classification2_loss: 0.0361 - val_classification1_accuracy: 0.7500 - val_classification2_accuracy: 0.9917\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 233us/step - loss: 0.3534 - classification1_loss: 0.3334 - classification2_loss: 0.0198 - classification1_accuracy: 0.8600 - classification2_accuracy: 0.9979 - val_loss: 0.4937 - val_classification1_loss: 0.4757 - val_classification2_loss: 0.0191 - val_classification1_accuracy: 0.7667 - val_classification2_accuracy: 0.9983\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 182us/step - loss: 0.3435 - classification1_loss: 0.3289 - classification2_loss: 0.0141 - classification1_accuracy: 0.8536 - classification2_accuracy: 0.9993 - val_loss: 0.5258 - val_classification1_loss: 0.5123 - val_classification2_loss: 0.0154 - val_classification1_accuracy: 0.7767 - val_classification2_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 183us/step - loss: 0.2904 - classification1_loss: 0.2808 - classification2_loss: 0.0094 - classification1_accuracy: 0.8829 - classification2_accuracy: 1.0000 - val_loss: 0.5261 - val_classification1_loss: 0.5128 - val_classification2_loss: 0.0136 - val_classification1_accuracy: 0.7533 - val_classification2_accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 197us/step - loss: 0.2487 - classification1_loss: 0.2413 - classification2_loss: 0.0067 - classification1_accuracy: 0.8936 - classification2_accuracy: 1.0000 - val_loss: 0.5436 - val_classification1_loss: 0.5326 - val_classification2_loss: 0.0123 - val_classification1_accuracy: 0.7733 - val_classification2_accuracy: 0.9967\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 202us/step - loss: 0.2329 - classification1_loss: 0.2282 - classification2_loss: 0.0048 - classification1_accuracy: 0.9086 - classification2_accuracy: 1.0000 - val_loss: 0.5326 - val_classification1_loss: 0.5213 - val_classification2_loss: 0.0117 - val_classification1_accuracy: 0.7617 - val_classification2_accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 187us/step - loss: 0.2023 - classification1_loss: 0.1978 - classification2_loss: 0.0043 - classification1_accuracy: 0.9264 - classification2_accuracy: 1.0000 - val_loss: 0.5510 - val_classification1_loss: 0.5398 - val_classification2_loss: 0.0117 - val_classification1_accuracy: 0.7600 - val_classification2_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 182us/step - loss: 0.2063 - classification1_loss: 0.2026 - classification2_loss: 0.0032 - classification1_accuracy: 0.9186 - classification2_accuracy: 1.0000 - val_loss: 0.5772 - val_classification1_loss: 0.5685 - val_classification2_loss: 0.0095 - val_classification1_accuracy: 0.7450 - val_classification2_accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss={'classification1': 'binary_crossentropy', \n",
    "                    'classification2': 'binary_crossentropy'},\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit([train_x,train_x2],[train_y,train_y2],\n",
    "                    validation_data=([test_x,test_x2],[test_y,test_y2]),\n",
    "                                     batch_size=32, nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "#create_download_link(filename='file.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf aclImdb\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
