{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/epfml/sent2vec\r\n",
      "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-klkg4fel\r\n",
      "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-klkg4fel\r\n",
      "Building wheels for collected packages: sent2vec\r\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1137401 sha256=80df24acfedcfa42c7400a4403d632d1740b391296d814505a5f30d86c7772dc\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k5nsuqff/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\r\n",
      "Successfully built sent2vec\r\n",
      "Installing collected packages: sent2vec\r\n",
      "Successfully installed sent2vec-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "# install sent2vec\n",
    "!pip install git+https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n",
    "\n",
    "Latest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download IMDB Movie Review Dataset\n",
    "Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\r\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "## Read-in the reviews and print some basic descriptions of them\n",
    "\n",
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization, Stop-word and Punctuation Removal Functions\n",
    "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary overarching hyperparameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000 # number of samples to generate in each class - 'positive', 'negative'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use regular expressions to remove unnecessary characters**\n",
    "\n",
    "Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[\\W\\d]', \" \", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop-word removal**\n",
    "\n",
    "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "# these stopwords may indicate positivity/negativity of sentiment, so we remove them (keep them in corpus)\n",
    "# stopwords.remove(\"no\")\n",
    "# stopwords.remove(\"nor\")\n",
    "# stopwords.remove(\"not\")\n",
    "\n",
    "# print(stopwords) # see default stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Embedding Vectors\n",
    "\n",
    "The following functions are used to extract sent2vec embedding vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sent2vec embedding took 10 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sent2vec\n",
    "\n",
    "s2v_model = sent2vec.Sent2vecModel()\n",
    "start=time.time()\n",
    "s2v_model.load_model('../input/sent2vec/wiki_unigrams.bin')\n",
    "end = time.time()\n",
    "print(\"Loading the sent2vec embedding took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_embedding_vectors(data):\n",
    "    out = None\n",
    "    for item in data:\n",
    "        vec = s2v_model.embed_sentence(\" \".join(str(i) for i in item if i))\n",
    "        if vec is not None:\n",
    "            if out is not None:\n",
    "                out = np.concatenate((out,vec),axis=0)\n",
    "            else:\n",
    "                out = vec                                            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together To Assemble Dataset\n",
    "\n",
    "Now, putting all the preprocessing steps together we assemble our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = remove_reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    \n",
    "    return data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::data_train::\n",
      "[list(['made', 'year', ' vertigo  ', 'equally', 'bewitching', 'movie ', 'though', 'much', 'lighter', 'vein ', 'it s', 'set', 'enchanted', 'new', 'york', 'winter ', 'kim', 'novak', 'witch', 'casts', 'spell', 'james', 'stewart ', 'gets', 'caught', 'instead ', 'the', 'interesting', 'sidelight', 'novak s', 'rival', 'played', 'janice', 'rule ', 'originated', 'part', 'madge', ' picnic ', 'broadway', ' the', 'part', 'novak', 'would', 'make', 'famous', 'film  '])\n",
      " list(['just', 'kidding  br', '   br', '  seeking', 'greener', 'pastures', 'form', 'hustling', 'new', 'york', 'city ', 'jon', 'voight', 'young', 'optimist', 'cowboy', ' almost', 'forest', 'gump like ', 'joe', 'buck', 'texas ', 'it', 'take', 'long', 'big', 'apple', 'mercilessly', 'swallow', 'ambitions', 'whole', 'soon', 'joe', 'target', 'coldness', 'new', 'yorkers', 'cons', 'street thugs ', 'given', 'pure', 'heart ', 'takes', 'pity', 'one', 'thugs ', 'ratso', 'rizzo', ' dustin', 'hoffman ', 'later', 'moves', 'wreck', 'apartment', 'two', 'literally', 'struggle', 'survive  br', '   br', '  while', 'midnight', 'comedy', 'labeled', 'drama ', 'best', 'described', 'either', 'tragic', 'comedy', 'comedic', 'tragedy', 'opinion ', 'it', 'beautiful', 'film', 'stylish', 'capturing', 'contemporary', 'hippie vibe', 'late', '    s', 'mandatory', 'dizzying', 'warhol party', 'cinematography', 'juxtaposing', 'ultra urban', 'new', 'york', 'city ', 'the', 'film', 'crams', 'cowboy', 'joe', 'buck', 'somewhere', 'between ', 'thereby', 'emphasizing', 'out of place', 'position ', 'we', 'feel', 'struggle', 'fit', 'in ', 'also', 'merely', 'get', 'enough', 'money', 'feed', 'ratso', 'rizzo  br', '   br', '  midnight', 'cowboy', 'brought', 'tears', 'eyes'])\n",
      " list(['everyone', 'knows', '  zero', 'day  ', 'event ', 'what', 'i', 'think', 'movie', 'elephant', 'made', 'us', 'see', 'guys', 'were ', 'they', 'showed', 'life', 'year ', 'throughout', 'movie', 'get', 'like', 'them ', 'laugh', 'even', 'though', 'totally', 'know', 'they re', 'gonna', 'do ', 'and', 'that', 'gives', 'chills ', 'cause', 'i', 'felt', 'guilty', 'cheered', 'comments ', 'i', 'thought', 'cal', 'sweet', 'guy ', 'even', 'though', 'i', 'knew', 'gonna', 'happen', 'know ', 'even', 'end', 'movie', 'commit', 'suicide', 'deciding', 'count', ' ', ' ', 'i', 'thought', 'funny', 'still', 'i', 'horrified', 'see', 'heads', 'blown', 'off ', 'of', 'course', 'i', 'was ', 'i', 'got', 'like', 'them ', 'they', 'wicked ', 'maybe ', 'i', 'felt', 'like', 'really', 'normal', 'guys ', 'really', 'realize', 'it ', 'but', 'i', 'knew', 'were  br', '   br', '  that s ', 'imo ', 'main', 'force', 'movie ', 'it', 'makes', 'us', 'realize', 'friends ', 'relatives ', 'anyone ', 'planning', 'something', 'crazy ', 'even', 'notice', 'it '])\n",
      " ...\n",
      " list(['being', 'fan', 'billy', 'bob', 'thornton ', 'diversity', 'skills ', 'i', 'noticed', 'movie', 'listed ', 'surprised', 'i', 'heard', 'it  br', '   br', '  i d', 'traveled', 'usual', 'period', 'filmed', '     ', 'hit', 'theaters', '     ', 'years', 'later', ' that', 'passage', 'time', 'first', 'clue', 'well', 'production   br', '   br', '  now', 'patrick', 'swayze', 'can t', 'act', 'sour', 'apples ', 'thornton', 'enough', 'ability', 'make up', 'difference', 'them ', 'and', 'charlize', 'theron', 'someone', 'would', 'pleasure', 'see ', 'even', 'showed', 'watching', 'paint', 'dry  br', '   br', '  being', 'curious ', 'i', 'checked', 'site s', 'production', 'info ', 'it', 'made', 'whopping', ' ', '    ', 'per', 'screen', 'opening', 'weekend ', '    ', 'each ', 'month s', 'theater', 'run', 'latter', '     ', 'overall', 'gross', '    k ', 'i d', 'doubt', 'could', 'cover', 'cast', 'crew s', 'hotel', 'food', 'week', 'location  br', '   br', '  the', 'story', 'pretty', 'benign ', 'even', 'use', 'usually', 'interesting', 'locale', 'reno', 'dull', 'rest', 'goings on  br', '   br', '  it s', 'something', 'like', 'several', 'snl', 'bits', 'pieced', 'together '])\n",
      " list(['many', 'people', 'know', 'feels', 'loved', 'one', 'lost ', 'the', 'feelings', 'pain ', 'grief', 'sorrow', 'unbearable ', 'however ', 'sometimes', 'memories', 'leave', 'behind', 'trigger', 'saddest', 'emotions ', 'this', 'theme', 'superbly', 'portrayed', 'short', 'film', ' tulip  ', 'directed', 'award', 'winning', 'australian', 'actress ', 'rachel', 'griffths ', 'described', 'movie', ' as', 'much', 'memories', 'love  ', 'string', 'sensitivity', 'sentimentality', 'expertly', 'threaded', 'triumphant', '  ', 'minute', 'film  br', '   br', '   tulip ', 'beautifully', 'wrought ', 'touching', 'heart warming', 'story', 'man s', 'journey', 'coming', 'terms', 'loss', 'wife', 'relationship', 'shares', 'special', 'animal ', ' tulip  ', 'the', 'film', 'opens', 'rising', 'dawn ', 'chirping', 'birds', 'vast', 'landscape ', 'introducing', 'sense', 'rustic', 'harmony', 'present', 'throughout', 'film ', 'a', 'soft', 'music', 'plays ', 'marking', 'entrance', 'ruth', ' jean', 'bain  ', 'she', 'wears', 'flowered', 'dress', 'apron', 'sun', 'hat', 'head ', 'she', 'gently', 'pets', 'tulip ', 'caressing', 'ears', 'patting', 'back ', 'the', 'furnishing', 'house', 'impressive', 'attention', 'detail', 'creditable', ' a', 'vase', 'tulips', 'seen'])\n",
      " list(['probably', 'somebody', 'heard', 'alberto', 'tomba ', 'a', 'former', 'policeman ', 'former', 'sky', 'champion ', 'and ', 'now ', 'terrible', 'actor ', ' alex', 'l ariete ', 'planned', 'tv', ' mini', 'serial  ', 'italian', 'television', 'refused', 'show', 'movie', 'channels ', 'now', 'a ', 'believe', 'me ', 'ridiculous', 'movie ', 'the', 'script', 'simply', 'hilarious', ' it s', 'supposed', 'dramatic', 'movie  ', 'something', 'like', ' ', 'years', 'old', 'kid', 'work ', 'but', 'really', 'blows', 'away', 'amateurish', 'acting ', 'alberto', 'tomba ', 'actually', 'believable', 'policeman', 'himself ', 'plays', 'terribly', 'totally', 'silly', 'character ', 'special', 'operations', 'italian', 'policeman', 'specialized', 'smashing', 'doors', 'open ', '  ariete ', ' ram   ', 'this', 'super guy', 'try', 'save', 'young', 'nice', 'girl', 'life', ' an', 'actual', 'italian', ' little ', 'tv', 'showgirl ', 'married', 'singer', 'eros', 'ramazzotti  ', 'nice', 'absolutely', 'inept', 'acting ', 'lose', 'one', 'make', 'favour ', 'a', 'movie', 'shame', 'italian', 'cinema', 'industry ', 'john', 'travolta', 'earth', 'attack', 'got', 'close  '])]\n"
     ]
    }
   ],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]\n",
    "\n",
    "del raw_data, raw_header # huge and no longer needed, get rid of it\n",
    "\n",
    "print(\"DEBUG::data_train::\")\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1033  967]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1865912   0.00296916 -0.08566295 ...  0.0182172  -0.01269061\n",
      "   0.21264666]\n",
      " [-0.07407703 -0.08794717  0.0178676  ...  0.04186484 -0.08544751\n",
      "   0.03688974]\n",
      " [ 0.01704564 -0.10370536 -0.0200744  ...  0.03406874 -0.10187364\n",
      "   0.17778794]\n",
      " ...\n",
      " [ 0.12999517 -0.05254492  0.18705492 ...  0.0082718  -0.03404788\n",
      "   0.06047222]\n",
      " [-0.07268659 -0.00444807 -0.02116374 ...  0.04578894  0.00873205\n",
      "   0.0256112 ]\n",
      " [ 0.02657777 -0.03309947 -0.12364911 ...  0.07154018 -0.06795725\n",
      "   0.10368899]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(data_train)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x/train_y list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[-0.1865912   0.00296916 -0.08566295 ...  0.0182172  -0.01269061\n",
      "   0.21264666]\n",
      " [-0.07407703 -0.08794717  0.0178676  ...  0.04186484 -0.08544751\n",
      "   0.03688974]\n",
      " [ 0.01704564 -0.10370536 -0.0200744  ...  0.03406874 -0.10187364\n",
      "   0.17778794]\n",
      " ...\n",
      " [ 0.14878118 -0.05725068  0.1905301  ...  0.12106173  0.10261593\n",
      "  -0.00295256]\n",
      " [ 0.13707681 -0.08506337  0.17267852 ...  0.01870527  0.07860093\n",
      "   0.07494058]\n",
      " [ 0.02448745  0.09204672 -0.02318908 ...  0.11524606  0.03332767\n",
      "   0.06049296]]\n",
      "[1 1 1 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "data = EmbeddingVectors\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = np.array(header[:idx])\n",
    "# # remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = np.array(header[idx:]) \n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Shallow Model for IMDB Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "input_shape = (len(train_x[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.1)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 415us/step - loss: 0.5649 - accuracy: 0.7050 - val_loss: 0.4669 - val_accuracy: 0.7767\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 172us/step - loss: 0.4055 - accuracy: 0.8186 - val_loss: 0.4193 - val_accuracy: 0.8050\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 170us/step - loss: 0.3576 - accuracy: 0.8364 - val_loss: 0.4070 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 153us/step - loss: 0.3258 - accuracy: 0.8636 - val_loss: 0.4096 - val_accuracy: 0.8167\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.2856 - accuracy: 0.8771 - val_loss: 0.4106 - val_accuracy: 0.8183\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 146us/step - loss: 0.2634 - accuracy: 0.8943 - val_loss: 0.4150 - val_accuracy: 0.8150\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 151us/step - loss: 0.2434 - accuracy: 0.9000 - val_loss: 0.4242 - val_accuracy: 0.8150\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 147us/step - loss: 0.2383 - accuracy: 0.9000 - val_loss: 0.4572 - val_accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 146us/step - loss: 0.2100 - accuracy: 0.9129 - val_loss: 0.4799 - val_accuracy: 0.8017\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 148us/step - loss: 0.1811 - accuracy: 0.9286 - val_loss: 0.4549 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model on Book Reviews from MDSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books.negative.review  dvd.negative.review\r\n",
      "books.positive.review  dvd.positive.review\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/multi-domain-sentiment-dataset-books-and-dvds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_MDSD(data):\n",
    "    out_lst = []\n",
    "    for i in range(len(data)):\n",
    "        txt = \"\"\n",
    "        if(data[i]==\"<review_text>\\n\"):\n",
    "            j=i\n",
    "            while(data[j]!=\"</review_text>\\n\"):\n",
    "                txt = txt+data[j]\n",
    "                j = j+1\n",
    "            text = tokenize(txt)\n",
    "            text = stop_word_removal(text)\n",
    "            text = remove_reg_expressions(text)\n",
    "            out_lst.append(text)\n",
    "            \n",
    "            #print(txt)\n",
    "            #print(text)\n",
    "            \n",
    "    return out_lst\n",
    "\n",
    "with open (\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.negative.review\", \"r\", encoding=\"latin1\") as myfile:\n",
    "    data=myfile.readlines()\n",
    "neg_books = parse_MDSD(data)\n",
    "len(neg_books)\n",
    "\n",
    "with open (\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.positive.review\", \"r\", encoding=\"latin1\") as myfile:\n",
    "    data=myfile.readlines()\n",
    "pos_books = parse_MDSD(data)\n",
    "len(pos_books)\n",
    "\n",
    "#print(neg_books)\n",
    "#print(pos_books)\n",
    "\n",
    "header = [0]*len(neg_books)\n",
    "header.extend([1]*len(pos_books))\n",
    "neg_books.extend(pos_books)\n",
    "MDSD_data = np.array(neg_books)\n",
    "\n",
    "data, sentiments = unison_shuffle_data(np.array(MDSD_data), header)\n",
    "\n",
    "len(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try using the IMDB classifier directly on book review data...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02857956 -0.19712123 -0.03367234 ...  0.16281778 -0.05813418\n",
      "   0.03944178]\n",
      " [-0.03060782 -0.07786036  0.02195755 ...  0.07094602 -0.13154835\n",
      "  -0.0274305 ]\n",
      " [-0.00442837 -0.11807001  0.07095549 ... -0.04005149 -0.12931705\n",
      "   0.17400824]\n",
      " ...\n",
      " [-0.09499011  0.02815587  0.01926375 ... -0.05984141 -0.09697013\n",
      "   0.20014554]\n",
      " [-0.13298799 -0.12199261 -0.07488126 ...  0.08148796 -0.14331028\n",
      "   0.12039624]\n",
      " [-0.11523205 -0.09352691  0.04332658 ...  0.12436987 -0.1109891\n",
      "   0.03355481]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(data)\n",
    "print(EmbeddingVectors)\n",
    "sentiments = np.asarray(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 38us/step\n",
      "[0.6671734366416932, 0.7440000176429749]\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.evaluate(x=EmbeddingVectors,y=sentiments)) # evaluate IMDB classifier on books directly\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of Book Review Domain via Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "encoding_dim = 30 # chosen experimentally\n",
    "\n",
    "input_shape = (len(train_x[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "encoder = Dense(encoding_dim)(sent2vec_vectors)\n",
    "dropout = Dropout(0.1)(encoder)\n",
    "decoder = Dense(encoding_dim)(dropout)\n",
    "dropout = Dropout(0.1)(decoder)\n",
    "output = Dense(len(train_x[0]))(dropout)\n",
    "autoencoder = Model(inputs=sent2vec_vectors, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 0s 263us/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0932 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0750\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0743 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0707\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0714 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0692\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0695 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0675\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0679 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0662\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0667 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0653\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0658 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0650\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0651 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0640\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0646 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0638\n",
      "Epoch 10/50\n",
      "1400/1400 [==============================] - 0s 79us/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0641 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0632\n",
      "Epoch 11/50\n",
      "1400/1400 [==============================] - 0s 80us/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0636 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0629\n",
      "Epoch 12/50\n",
      "1400/1400 [==============================] - 0s 79us/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0632 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0626\n",
      "Epoch 13/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0628 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0624\n",
      "Epoch 14/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0626 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0621\n",
      "Epoch 15/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0623 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0619\n",
      "Epoch 16/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0621 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0617\n",
      "Epoch 17/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0615\n",
      "Epoch 18/50\n",
      "1400/1400 [==============================] - 0s 80us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0617 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0613\n",
      "Epoch 19/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0616 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0612\n",
      "Epoch 20/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0613 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0610\n",
      "Epoch 21/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0613 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0608\n",
      "Epoch 22/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0610 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0607\n",
      "Epoch 23/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0610 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0606\n",
      "Epoch 24/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0608 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0604\n",
      "Epoch 25/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0603\n",
      "Epoch 26/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0605 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0601\n",
      "Epoch 27/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0604 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0601\n",
      "Epoch 28/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0603 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0599\n",
      "Epoch 29/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0602 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0598\n",
      "Epoch 30/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0597\n",
      "Epoch 31/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0597\n",
      "Epoch 32/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0599 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0596\n",
      "Epoch 33/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0599 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0595\n",
      "Epoch 34/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0597 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0594\n",
      "Epoch 35/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0597 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0594\n",
      "Epoch 36/50\n",
      "1400/1400 [==============================] - 0s 79us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0597 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0593\n",
      "Epoch 37/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0597 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0592\n",
      "Epoch 38/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0596 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0592\n",
      "Epoch 39/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0595 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0591\n",
      "Epoch 40/50\n",
      "1400/1400 [==============================] - 0s 76us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0591\n",
      "Epoch 41/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0591\n",
      "Epoch 42/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0590\n",
      "Epoch 43/50\n",
      "1400/1400 [==============================] - 0s 86us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0593 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0590\n",
      "Epoch 44/50\n",
      "1400/1400 [==============================] - 0s 83us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0590\n",
      "Epoch 45/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0593 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0589\n",
      "Epoch 46/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0592 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0589\n",
      "Epoch 47/50\n",
      "1400/1400 [==============================] - 0s 81us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0592 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0589\n",
      "Epoch 48/50\n",
      "1400/1400 [==============================] - 0s 87us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0591 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0588\n",
      "Epoch 49/50\n",
      "1400/1400 [==============================] - 0s 78us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0592 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0588\n",
      "Epoch 50/50\n",
      "1400/1400 [==============================] - 0s 77us/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0591 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd0d737d320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam',loss='mse',metrics=[\"mse\",\"mae\"])\n",
    "autoencoder.fit(train_x,train_x,validation_data=(test_x, test_x), batch_size=32,\n",
    "                    nb_epoch=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 600)\n",
      "2000/2000 [==============================] - 0s 38us/step\n",
      "[0.5876209111213684, 0.75]\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "# transform EmbeddingVectors and sentiments with autoencoder.predict and then evaluate IDMB model again\n",
    "\n",
    "EmbeddingVectorsProjected = autoencoder.predict(EmbeddingVectors)\n",
    "\n",
    "print(EmbeddingVectorsProjected.shape)\n",
    "\n",
    "print(model.evaluate(x=EmbeddingVectorsProjected,y=sentiments))\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "#create_download_link(filename='GBMimportances.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf aclImdb\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
