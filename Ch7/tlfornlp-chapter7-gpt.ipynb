{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","metadata":{}},{"cell_type":"markdown","source":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp","metadata":{}},{"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Open Ended Text Generation with GPT-2","metadata":{}},{"cell_type":"code","source":"# Pipeline uses `gpt2` by default, but we specify it explicitly to be fully transparent\nfrom transformers import pipeline\ngpt = pipeline('text-generation',model='gpt2')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a178bc973b42359e8ee980162c520a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4435c74d817041cea86cfe666469a36f"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5fe0ff35ca6453ba9ead5140e2cde72"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd5ca4a2f1948debca0b13e7d2c3fc9"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eedacf10b3a74ebb99e2516f08151eff"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, let's generate some text with GPT-2","metadata":{}},{"cell_type":"code","source":"gpt(\"Transfer learning is a field of study\", max_length=100)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"Transfer learning is a field of study that combines the field with the community's knowledge. We train new students to recognize, manage, and respond to the many problems facing this special role in their daily lives: interpersonal issues, physical and mental pain, and general health. Teaching students to interact with and understand each other is also an important part of our profession. As part of our education and community outreach work, we have launched programs to help prepare young adults to become leaders in a fast-paced and interactive\"}]"},"metadata":{}}]},{"cell_type":"markdown","source":"A nonexhaustive list of other model choices suitable for text generation within the transformers library include \"ctrl\" (CTRL - huge! too big for Kaggle), \"xlnet-base-cased\" (XLNet), \"transfo-xl-wt103\" (Transformer XL)... These often need to be padded very carefully to work well, GPT-2 is the safest choice for open-ended text generation. See https://huggingface.co/transformers/usage.html#text-generation for more.","metadata":{}},{"cell_type":"markdown","source":"# Conversational Text Generation with DialoGPT\n\nDialoGPT is an extension of GPT to conversational response generation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer # you can use these utility classes that automatically load the right classes\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer # or these more specific classes directly\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\")","metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda51eb8825b4a9db110cf6b99c2e7cf"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89225db69ea4c4ea79ba06dbea0034a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a258467d3d264189bed3ecafcf9421b3"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=862955157.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bcdedab26564e2bb2baa4499321fc5b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Chat for 5 Lines\nconversation_length = 5\nfor step in range(conversation_length):\n    # encode new user input, add end-of-sentence token, return tensor\n    new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors='pt')\n    \n    # add new input to chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1) if step > 0 else new_user_inputs_ids\n    \n    # generate a response of up to max_length tokens\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    \n    # display response\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","metadata":{"trusted":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdin","text":"User:  hi robot\n"},{"name":"stdout","text":"DialoGPT: Hello, human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  huh?\n"},{"name":"stdout","text":"DialoGPT: I'm a bot.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  ok, what is your name?\n"},{"name":"stdout","text":"DialoGPT: Robot. I'm a robot.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  ok! Can you say something else?\n"},{"name":"stdout","text":"DialoGPT: Robot. I'm a robot.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  Do you have children?\n"},{"name":"stdout","text":"DialoGPT: Robot. I'm a robot.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Because the notebook is not saving input text, here is the entire conversation so you know how we prompted the model**\n\nUser:  hi robot\n\nDialoGPT: Hello, human.\n\nUser:  huh?\n\nDialoGPT: I'm a bot.\n\nUser:  ok, what is your name?\n\nDialoGPT: Robot. I'm a robot.\n\nUser:  ok! Can you say something else?\n\nDialoGPT: Robot. I'm a robot.\n\nUser:  Do you have children?\n\nDialoGPT: Robot. I'm a robot.","metadata":{}}]}